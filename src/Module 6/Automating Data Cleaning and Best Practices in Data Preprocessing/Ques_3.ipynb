{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Practices for Data Preprocessing\n",
    "\n",
    "#### Always Explore & Visualize Data First"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values per column:\n",
      " MedInc        100\n",
      "HouseAge        0\n",
      "AveRooms        0\n",
      "AveBedrms       0\n",
      "Population      0\n",
      "AveOccup        0\n",
      "Latitude        0\n",
      "Longitude       0\n",
      "dtype: int64\n",
      "\n",
      "Summary statistics (to detect outliers):\n",
      "              MedInc      HouseAge      AveRooms     AveBedrms    Population  \\\n",
      "count  20540.000000  20640.000000  20640.000000  20640.000000  20640.000000   \n",
      "mean       3.878913     28.639486      5.429000      1.096675   1425.476744   \n",
      "std        1.898162     12.585558      2.474173      0.473911   1132.462122   \n",
      "min        0.499900      1.000000      0.846154      0.333333      3.000000   \n",
      "25%        2.570250     18.000000      4.440716      1.006079    787.000000   \n",
      "50%        3.542150     29.000000      5.229129      1.048780   1166.000000   \n",
      "75%        4.750000     37.000000      6.052381      1.099526   1725.000000   \n",
      "max       15.000100     52.000000    141.909091     34.066667  35682.000000   \n",
      "\n",
      "           AveOccup      Latitude     Longitude  \n",
      "count  20640.000000  20640.000000  20640.000000  \n",
      "mean       3.070655     35.631861   -119.569704  \n",
      "std       10.386050      2.135952      2.003532  \n",
      "min        0.692308     32.540000   -124.350000  \n",
      "25%        2.429741     33.930000   -121.800000  \n",
      "50%        2.818116     34.260000   -118.490000  \n",
      "75%        3.282261     37.710000   -118.010000  \n",
      "max     1243.333333     41.950000   -114.310000  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "data = fetch_california_housing(as_frame=True)\n",
    "df = data.frame\n",
    "df.iloc[0:100, 0] = np.nan  \n",
    "X = df.drop(columns='MedHouseVal')\n",
    "y = df['MedHouseVal']\n",
    "missing_values = X.isnull().sum()\n",
    "print(\"Missing values per column:\\n\", missing_values)\n",
    "print(\"\\nSummary statistics (to detect outliers):\\n\", X.describe())\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', StandardScaler())\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handle Missing & Inconsistent Data Before Applying ML Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape after dropping missing values: (20540, 8)\n",
      "Shape after imputation: (20640, 8)\n",
      "Outliers handled via capping.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "data = fetch_california_housing(as_frame=True)\n",
    "df = data.frame\n",
    "df.iloc[0:100, 0] = np.nan \n",
    "X = df.drop(columns='MedHouseVal')\n",
    "y = df['MedHouseVal']\n",
    "X_dropped = X.dropna()\n",
    "y_dropped = y[X_dropped.index] \n",
    "print(\"Shape after dropping missing values:\", X_dropped.shape) \n",
    "from sklearn.impute import SimpleImputer\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X_filled = imputer.fit_transform(X)\n",
    "print(\"Shape after imputation:\", X_filled.shape)\n",
    "def cap_outliers(df, lower_percentile=0.01, upper_percentile=0.99):\n",
    "    capped_df = df.copy()\n",
    "    for col in df.columns:\n",
    "        lower = df[col].quantile(lower_percentile)\n",
    "        upper = df[col].quantile(upper_percentile)\n",
    "        capped_df[col] = np.clip(df[col], lower, upper)\n",
    "    return capped_df\n",
    "X_capped = cap_outliers(pd.DataFrame(X_filled, columns=X.columns))\n",
    "print(\"Outliers handled via capping.\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose the Right Scaling Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   feature1  feature2\n",
      "0      0.00      0.00\n",
      "1      0.25      0.25\n",
      "2      0.50      0.50\n",
      "3      0.75      0.75\n",
      "4      1.00      1.00\n",
      "   feature1  feature2\n",
      "0      -1.0      -1.0\n",
      "1      -0.5      -0.5\n",
      "2       0.0       0.0\n",
      "3       0.5       0.5\n",
      "4      23.5       1.0\n",
      "   feature1  feature2\n",
      "0 -1.000000       0.2\n",
      "1 -0.666667       0.4\n",
      "2  0.000000       0.6\n",
      "3  0.666667       0.8\n",
      "4  1.000000       1.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pandas as pd\n",
    "data = {'feature1': [10, 20, 30, 40, 50],\n",
    "        'feature2': [1, 2, 3, 4, 5]}\n",
    "df = pd.DataFrame(data)\n",
    "scaler = MinMaxScaler()\n",
    "scaled_data = scaler.fit_transform(df)\n",
    "scaled_df = pd.DataFrame(scaled_data, columns=df.columns)\n",
    "print(scaled_df)\n",
    "\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "data = {'feature1': [10, 20, 30, 40, 500],  \n",
    "        'feature2': [1, 2, 3, 4, 5]}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "scaler = RobustScaler()\n",
    "scaled_data = scaler.fit_transform(df)\n",
    "scaled_df = pd.DataFrame(scaled_data, columns=df.columns)\n",
    "\n",
    "print(scaled_df)\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "data = {'feature1': [-30, -20, 0, 20, 30],\n",
    "        'feature2': [1, 2, 3, 4, 5]}\n",
    "df = pd.DataFrame(data)\n",
    "scaler = MaxAbsScaler()\n",
    "scaled_data = scaler.fit_transform(df)\n",
    "scaled_df = pd.DataFrame(scaled_data, columns=df.columns)\n",
    "print(scaled_df)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keep Track of Data Transformations for Reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.   0.  ]\n",
      " [0.25 0.25]\n",
      " [0.5  0.5 ]\n",
      " [0.75 0.75]\n",
      " [1.   1.  ]]\n",
      "[[-1.  -1. ]\n",
      " [-0.5 -0.5]\n",
      " [ 0.   0. ]\n",
      " [ 0.5  0.5]\n",
      " [ 1.   1. ]]\n",
      "[[0.2 0.2]\n",
      " [0.4 0.4]\n",
      " [0.6 0.6]\n",
      " [0.8 0.8]\n",
      " [1.  1. ]]\n",
      "[10.  1.] [50.  5.]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import logging\n",
    "from sklearn.preprocessing import MinMaxScaler, RobustScaler, MaxAbsScaler\n",
    "import pandas as pd\n",
    "logging.basicConfig(filename='data_preprocessing.log', level=logging.INFO, format='%(asctime)s - %(message)s')\n",
    "data = {'feature1': [10, 20, 30, 40, 50],\n",
    "        'feature2': [1, 2, 3, 4, 5]}\n",
    "df = pd.DataFrame(data)\n",
    "def log_preprocessing_step(step_name, details):\n",
    "    logging.info(f\"Step: {step_name}, Details: {details}\")\n",
    "scaler_minmax = MinMaxScaler()\n",
    "scaler_robust = RobustScaler()\n",
    "scaler_maxabs = MaxAbsScaler()\n",
    "scaler_minmax.fit(df)\n",
    "scaled_data_minmax = scaler_minmax.transform(df)\n",
    "log_preprocessing_step(\"Min-Max Scaling\", {\"min\": scaler_minmax.data_min_, \"max\": scaler_minmax.data_max_})\n",
    "scaler_robust.fit(df)\n",
    "scaled_data_robust = scaler_robust.transform(df)\n",
    "log_preprocessing_step(\"Robust Scaling\", {\"median\": scaler_robust.center_, \"IQR\": scaler_robust.scale_})\n",
    "scaler_maxabs.fit(df)\n",
    "scaled_data_maxabs = scaler_maxabs.transform(df)\n",
    "log_preprocessing_step(\"MaxAbs Scaling\", {\"max_abs\": scaler_maxabs.max_abs_})\n",
    "print(scaled_data_minmax)\n",
    "print(scaled_data_robust)\n",
    "print(scaled_data_maxabs)\n",
    "\n",
    "#.\n",
    "import pickle\n",
    "def store_transformation_parameters(scaler, filename):\n",
    "    with open(filename, 'wb') as file:\n",
    "        pickle.dump(scaler, file)\n",
    "store_transformation_parameters(scaler_minmax, 'minmax_scaler.pkl')\n",
    "store_transformation_parameters(scaler_robust, 'robust_scaler.pkl')\n",
    "store_transformation_parameters(scaler_maxabs, 'maxabs_scaler.pkl')\n",
    "def load_transformation_parameters(filename):\n",
    "    with open(filename, 'rb') as file:\n",
    "        scaler = pickle.load(file)\n",
    "    return scaler\n",
    "loaded_scaler_minmax = load_transformation_parameters('minmax_scaler.pkl')\n",
    "print(loaded_scaler_minmax.data_min_, loaded_scaler_minmax.data_max_)\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
