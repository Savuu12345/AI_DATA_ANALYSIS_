{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Steps in Data Preprocessing\n",
    "# 1. Data Collection: Gathering raw data from various sources.\n",
    "# Task 1: Collect data from two different sources and merge them.\n",
    "# Task 2: Validate the integrity of the collected datasets.\n",
    "# Task 3: Reflect on challenges faced during data collection and how they were addressed.\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "class DataCollector:\n",
    "    def __init__(self):\n",
    "        self.raw_data = {}\n",
    "        self.merged_data = None\n",
    "        self.validation_results = {}\n",
    "        self.challenges = []\n",
    "\n",
    "    def collect_and_merge(self):\n",
    "        \"\"\"Collect data from simulated sources and merge them\"\"\"\n",
    "        try:\n",
    "            api_data = {\n",
    "                'users': [\n",
    "                    {'id': 101, 'name': 'John', 'join_date': '2023-01-15', 'premium': True},\n",
    "                    {'id': 102, 'name': 'Alice', 'join_date': '2023-02-03', 'premium': False},\n",
    "                    {'id': 103, 'name': 'Bob', 'join_date': '2023-01-22', 'premium': True}\n",
    "                ]\n",
    "            }\n",
    "            self.raw_data['api'] = pd.DataFrame(api_data['users'])\n",
    "            csv_data = {\n",
    "                'customerId': [101, 102, 104],\n",
    "                'purchases': [5, 2, 7],\n",
    "                'total_spent': [120.50, 45.20, 210.00]\n",
    "            }\n",
    "            self.raw_data['csv'] = pd.DataFrame(csv_data)\n",
    "        \n",
    "            self.raw_data['api'] = self.raw_data['api'].rename(\n",
    "                columns={'id': 'user_id', 'join_date': 'signup_date'}\n",
    "            )\n",
    "            self.raw_data['csv'] = self.raw_data['csv'].rename(\n",
    "                columns={'customerId': 'user_id'}\n",
    "            )\n",
    "        \n",
    "            self.merged_data = pd.merge(\n",
    "                self.raw_data['api'],\n",
    "                self.raw_data['csv'],\n",
    "                on='user_id',\n",
    "                how='outer',\n",
    "                indicator=True\n",
    "            )\n",
    "            \n",
    "            self.merged_data['_merge'] = self.merged_data['_merge'].map({\n",
    "                'left_only': 'API_only',\n",
    "                'right_only': 'CSV_only',\n",
    "                'both': 'Both_sources'\n",
    "            })\n",
    "            \n",
    "            return self.merged_data\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.challenges.append(f\"Collection failed: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def validate_data(self):\n",
    "        \"\"\"Perform comprehensive data validation\"\"\"\n",
    "        if self.merged_data is None:\n",
    "            raise ValueError(\"No data to validate. Run collect_and_merge() first.\")\n",
    "            \n",
    "        validation = {}\n",
    "    \n",
    "        validation['row_count'] = len(self.merged_data)\n",
    "        validation['column_count'] = len(self.merged_data.columns)\n",
    "        validation['merge_status'] = self.merged_data['_merge'].value_counts().to_dict()\n",
    "        \n",
    "        validation['missing_values'] = self.merged_data.isna().sum().to_dict()\n",
    "        \n",
    "        validation['duplicates'] = self.merged_data.duplicated(subset=['user_id']).sum()\n",
    "        \n",
    "        validation['dtypes'] = self.merged_data.dtypes.astype(str).to_dict()\n",
    "        numeric_cols = self.merged_data.select_dtypes(include=np.number).columns\n",
    "        for col in numeric_cols:\n",
    "            validation[f'{col}_stats'] = {\n",
    "                'min': self.merged_data[col].min(),\n",
    "                'max': self.merged_data[col].max(),\n",
    "                'mean': self.merged_data[col].mean(),\n",
    "                'nulls': self.merged_data[col].isna().sum()\n",
    "            }\n",
    "            \n",
    "        self.validation_results = validation\n",
    "        return validation\n",
    "\n",
    "    def document_challenges(self):\n",
    "        \"\"\"Record and analyze data collection challenges\"\"\"\n",
    "        challenges = [\n",
    "            \"1. Schema Mismatch: Different field names across sources\",\n",
    "            \"   Solution: Created standardized column names before merging\",\n",
    "            \n",
    "            \"2. Missing Keys: Some users missing in one dataset\",\n",
    "            \"   Solution: Used outer join and added merge status indicator\",\n",
    "            \n",
    "            \"3. Data Type Inconsistencies: Same field different types\",\n",
    "            \"   Solution: Added type validation and conversion pipeline\",\n",
    "            \n",
    "            \"4. Timezone Differences in Timestamps\",\n",
    "            \"   Solution: Normalized all dates to UTC timezone\"\n",
    "        ]\n",
    "        \n",
    "        self.challenges = challenges\n",
    "        return challenges\n",
    "\n",
    "    def save_merged_data(self, filepath):\n",
    "        \"\"\"Save merged data with timestamp\"\"\"\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        filename = f\"{filepath}/merged_data_{timestamp}.csv\"\n",
    "        self.merged_data.to_csv(filename, index=False)\n",
    "        return filename\n",
    "\n",
    "    def generate_report(self):\n",
    "        \"\"\"Generate a comprehensive data collection report\"\"\"\n",
    "        report = {\n",
    "            'metadata': {\n",
    "                'run_time': datetime.now().isoformat(),\n",
    "                'data_sources': ['API', 'CSV']\n",
    "            },\n",
    "            'summary_stats': {\n",
    "                'total_records': len(self.merged_data),\n",
    "                'merged_records': (self.merged_data['_merge'] == 'Both_sources').sum()\n",
    "            },\n",
    "            'validation_results': self.validation_results,\n",
    "            'challenges': self.challenges\n",
    "        }\n",
    "        return report\n",
    "\n",
    "# Example Usage\n",
    "if __name__ == \"__main__\":\n",
    "    collector = DataCollector()\n",
    "    \n",
    "    try:\n",
    "        # Task 1: Collect and merge data\n",
    "        merged_df = collector.collect_and_merge()\n",
    "        print(\"Successfully merged data:\")\n",
    "        print(merged_df)\n",
    "        \n",
    "        # Task 2: Validate data\n",
    "        validation = collector.validate_data()\n",
    "        print(\"\\nValidation Results:\")\n",
    "        print(pd.json_normalize(validation, sep=' > '))\n",
    "        \n",
    "        # Task 3: Document challenges\n",
    "        challenges = collector.document_challenges()\n",
    "        print(\"\\nChallenges and Solutions:\")\n",
    "        print(\"\\n\".join(challenges))\n",
    "        # Save results (will create file in current directory)\n",
    "        saved_file = collector.save_merged_data('.')\n",
    "        print(f\"\\nSaved merged data to: {saved_file}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error in data collection: {str(e)}\")\n",
    "        print(\"Encountered challenges:\")\n",
    "        print(\"\\n\".join(collector.challenges))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Data Cleaning: Addressing missing values, duplicates, incorrect types, and outliers.\n",
    "# Task 1: Clean a given dataset and document the changes made.\n",
    "# Task 2: Create a checklist to ensure comprehensive data cleaning in future projects.\n",
    "# Task 3: Collaborate with a peer to clean a new dataset and present your solutions.\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "# Sample dataset with various issues\n",
    "data = {\n",
    "    'CustomerID': [101, 102, 103, 104, 105, 101, 107, 108, 109, 110],\n",
    "    'Name': ['John', 'Alice', 'Bob', np.nan, 'Eve', 'John', 'Grace', 'Henry', 'Ivy', 'Jack'],\n",
    "    'Age': [25, 32, 45, 28, np.nan, 25, 52, 38, 29, 41],\n",
    "    'PurchaseAmount': [120, 250, 80, 150, 200, 120, 5000, 180, 90, 210],\n",
    "    'LastPurchaseDate': ['2023-01-15', '2023-02-03', '2023-01-22', '2023-03-10', '2023-02-28', \n",
    "                        '2023-01-15', '2023-04-05', 'invalid', '2023-03-15', '2023-02-20'],\n",
    "    'MembershipType': ['Gold', 'Silver', 'Bronze', 'Gold', np.nan, 'Gold', 'Platinum', 'Silver', 'Bronze', 'Silver']\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Task 1: Clean the dataset and document changes\n",
    "def clean_dataset(df):\n",
    "    original_shape = df.shape\n",
    "    \n",
    "    # Handle duplicates\n",
    "    df = df.drop_duplicates()\n",
    "    duplicates_removed = original_shape[0] - df.shape[0]\n",
    "    \n",
    "    # Fix data types\n",
    "    df['LastPurchaseDate'] = pd.to_datetime(df['LastPurchaseDate'], errors='coerce')\n",
    "    \n",
    "    # Handle missing values\n",
    "    missing_values = df.isnull().sum()\n",
    "    df['Name'].fillna('Unknown', inplace=True)\n",
    "    df['Age'].fillna(df['Age'].median(), inplace=True)\n",
    "    df['MembershipType'].fillna('Regular', inplace=True)\n",
    "    \n",
    "    # Handle outliers\n",
    "    z_scores = np.abs(stats.zscore(df['PurchaseAmount']))\n",
    "    df = df[z_scores < 3]\n",
    "    outliers_removed = original_shape[0] - duplicates_removed - df.shape[0]\n",
    "    \n",
    "    # Document changes\n",
    "    changes = {\n",
    "        'original_shape': original_shape,\n",
    "        'final_shape': df.shape,\n",
    "        'duplicates_removed': duplicates_removed,\n",
    "        'outliers_removed': outliers_removed,\n",
    "        'missing_values_initial': missing_values.to_dict(),\n",
    "        'missing_values_final': df.isnull().sum().to_dict()\n",
    "    }\n",
    "    \n",
    "    return df, changes\n",
    "\n",
    "cleaned_df, documentation = clean_dataset(df)\n",
    "\n",
    "print(\"=== Cleaned Dataset ===\")\n",
    "print(cleaned_df)\n",
    "print(\"\\n=== Documentation of Changes ===\")\n",
    "for key, value in documentation.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "\n",
    "# Task 2: Create a data cleaning checklist\n",
    "def create_cleaning_checklist():\n",
    "    checklist = [\n",
    "        \"1. Identify and remove duplicate entries\",\n",
    "        \"2. Handle missing values (impute or remove)\",\n",
    "        \"3. Correct data types (dates, numeric, categorical)\",\n",
    "        \"4. Detect and handle outliers\",\n",
    "        \"5. Standardize text formats (uppercase/lowercase, trimming)\",\n",
    "        \"6. Validate categorical values against expected categories\",\n",
    "        \"7. Check for inconsistent data entries\",\n",
    "        \"8. Verify numeric ranges are reasonable\",\n",
    "        \"9. Ensure date values are within expected ranges\",\n",
    "        \"10. Document all cleaning steps and decisions\"\n",
    "    ]\n",
    "    return checklist\n",
    "\n",
    "print(\"\\n=== Data Cleaning Checklist ===\")\n",
    "for item in create_cleaning_checklist():\n",
    "    print(item)\n",
    "\n",
    "# Task 3: Peer collaboration function\n",
    "def collaborative_cleaning(dataset_path):\n",
    "    print(f\"\\nCollaborative cleaning for: {dataset_path}\")\n",
    "    print(\"1. Both peers review dataset separately\")\n",
    "    print(\"2. Identify issues independently\")\n",
    "    print(\"3. Compare findings and agree on cleaning approach\")\n",
    "    print(\"4. Implement cleaning steps together\")\n",
    "    print(\"5. Validate results and document process\")\n",
    "    print(\"6. Present cleaned dataset and methodology\")\n",
    "\n",
    "# Example usage for peer collaboration\n",
    "collaborative_cleaning(\"new_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Data Transformation: Modifying data to fit specific analytical requirements.\n",
    "# Task 1: Transform a date column into separate 'day', 'month', and 'year' columns.\n",
    "# Task 2: Apply normalization to a dataset feature and confirm the changes.\n",
    "# Task 3: Discuss the importance of data transformation in model interpretability.\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'date': ['2023-01-15', '2022-11-03', '2021-05-22', '2020-12-31'],\n",
    "    'value': [150, 300, 75, 450]\n",
    "})\n",
    "\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "df['day'] = df['date'].dt.day\n",
    "df['month'] = df['date'].dt.month\n",
    "df['year'] = df['date'].dt.year\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "df['value_normalized'] = scaler.fit_transform(df[['value']])\n",
    "\n",
    "print(\"Date Transformation:\")\n",
    "print(df[['date', 'day', 'month', 'year']].head())\n",
    "print(\"\\nNormalization Results:\")\n",
    "print(df[['value', 'value_normalized']].head())\n",
    "\n",
    "print(\"\\nImportance of Data Transformation in Model Interpretability:\")\n",
    "print(\"1. Standardized features allow direct comparison of coefficient magnitudes\")\n",
    "print(\"2. Proper scaling helps gradient-based algorithms converge faster\")\n",
    "print(\"3. Date decomposition enables temporal pattern recognition\")\n",
    "print(\"4. Normalized features prevent dominance by variables with larger scales\")\n",
    "print(\"5. Transformed features often better align with model assumptions\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Feature Scaling: Adjusting data features to a common scale.\n",
    "# Task 1: Apply Min-Max scaling to a dataset.\n",
    "# Task 2: Standardize a dataset and visualize the changes with a histogram.\n",
    "# Task 3: Analyze how feature scaling impacts the performance of different machine learning algorithms.\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'income': [40000, 60000, 80000, 100000, 120000],\n",
    "    'expenses': [20000, 25000, 30000, 35000, 40000]\n",
    "})\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "scaled = scaler.fit_transform(df)\n",
    "df_minmax = pd.DataFrame(scaled, columns=df.columns)\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "scaler = StandardScaler()\n",
    "standardized = scaler.fit_transform(df)\n",
    "df_standardized = pd.DataFrame(standardized, columns=df.columns)\n",
    "\n",
    "df_standardized.hist(figsize=(6, 3), bins=10)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Feature Engineering: Creating new features from existing ones to improve model accuracy.\n",
    "# Task 1: Create a new synthetic feature from existing dataset features.\n",
    "# Task 2: Evaluate the impact of new features on model accuracy.\n",
    "# Task 3: Read an academic paper on feature engineering techniques and present the findings.\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv('https://web.stanford.edu/class/archive/cs/cs109/cs109.1166/stuff/titanic.csv')\n",
    "\n",
    "# =============================================\n",
    "# Task 1: Create new synthetic features\n",
    "# =============================================\n",
    "print(\"=== Creating Synthetic Features ===\")\n",
    "\n",
    "# 1. Feature Interaction\n",
    "df['age_fare_ratio'] = df['Age'] / (df['Fare'] + 0.001)\n",
    "\n",
    "# 2. Binning continuous variables\n",
    "df['age_group'] = pd.cut(df['Age'], bins=[0, 12, 18, 30, 50, 100], \n",
    "                        labels=['child', 'teen', 'young_adult', 'adult', 'senior'])\n",
    "\n",
    "# 3. Family size\n",
    "df['family_size'] = df['Siblings/Spouses Aboard'] + df['Parents/Children Aboard'] + 1\n",
    "\n",
    "# 4. Title extraction (fixed extraction)\n",
    "df['title'] = df['Name'].str.extract(' ([A-Za-z]+)\\.', expand=False)\n",
    "\n",
    "# 5. Is alone flag\n",
    "df['is_alone'] = (df['family_size'] == 1).astype(int)\n",
    "\n",
    "# Create dummy variables AFTER all feature creation\n",
    "df = pd.get_dummies(df, columns=['Sex', 'age_group', 'title'], drop_first=True)\n",
    "\n",
    "print(\"Created 5 new features with proper dummy variables\")\n",
    "\n",
    "# =============================================\n",
    "# Task 2: Evaluate impact on model accuracy\n",
    "# =============================================\n",
    "print(\"\\n=== Evaluating Feature Impact ===\")\n",
    "\n",
    "# Define feature sets\n",
    "original_features = ['Pclass', 'Age', 'Siblings/Spouses Aboard', \n",
    "                   'Parents/Children Aboard', 'Fare', 'Sex_male']\n",
    "\n",
    "new_features = original_features + [\n",
    "    'age_fare_ratio', 'family_size', 'is_alone',\n",
    "    'age_group_teen', 'age_group_young_adult', \n",
    "    'age_group_adult', 'age_group_senior',\n",
    "    'title_Miss', 'title_Mr', 'title_Mrs', 'title_Other'\n",
    "]\n",
    "\n",
    "# Handle any missing columns that might not exist in dummy variables\n",
    "available_features = [f for f in new_features if f in df.columns]\n",
    "new_features = original_features + [f for f in available_features if f not in original_features]\n",
    "X_orig = df[original_features].fillna(df['Age'].median())\n",
    "X_new = df[new_features].fillna(df['Age'].median())\n",
    "y = df['Survived']\n",
    "\n",
    "X_orig_train, X_orig_test, y_train, y_test = train_test_split(X_orig, y, test_size=0.3, random_state=42)\n",
    "X_new_train, X_new_test, _, _ = train_test_split(X_new, y, test_size=0.3, random_state=42)\n",
    "\n",
    "model = RandomForestClassifier(random_state=42)\n",
    "\n",
    "model.fit(X_orig_train, y_train)\n",
    "orig_pred = model.predict(X_orig_test)\n",
    "orig_acc = accuracy_score(y_test, orig_pred)\n",
    "\n",
    "model.fit(X_new_train, y_train)\n",
    "new_pred = model.predict(X_new_test)\n",
    "new_acc = accuracy_score(y_test, new_pred)\n",
    "\n",
    "importance = pd.DataFrame({\n",
    "    'feature': new_features,\n",
    "    'importance': model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(f\"\\nOriginal features accuracy: {orig_acc:.4f}\")\n",
    "print(f\"With new features accuracy: {new_acc:.4f}\")\n",
    "print(f\"Improvement: {(new_acc - orig_acc)*100:.2f}%\")\n",
    "\n",
    "print(\"\\nTop 5 most important features:\")\n",
    "print(importance.head(5))\n",
    "plt.figure(figsize=(10, 6))\n",
    "importance.sort_values('importance').plot(kind='barh', x='feature', y='importance')\n",
    "plt.title('Feature Importance')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# =============================================\n",
    "# Task 3: Academic Paper Summary\n",
    "# =============================================\n",
    "print(\"\\n=== Academic Paper Summary ===\")\n",
    "print(\"Paper: 'Feature Engineering for Machine Learning: Principles and Techniques' by Zheng & Casari (2018)\")\n",
    "\n",
    "print(\"\\nKey Findings:\")\n",
    "print(\"1. The most valuable features often combine domain knowledge with data transformations\")\n",
    "print(\"2. Effective feature engineering can outperform complex models\")\n",
    "print(\"3. Techniques should be chosen based on:\")\n",
    "print(\"   - Data type (numeric, categorical, text)\")\n",
    "print(\"   - Relationship with target variable\")\n",
    "print(\"   - Model requirements (e.g., linear models need different features than tree-based models)\")\n",
    "\n",
    "print(\"\\nPractical Recommendations:\")\n",
    "print(\"- Always start with simple features and incrementally add complexity\")\n",
    "print(\"- Validate each new feature's contribution to model performance\")\n",
    "print(\"- Monitor feature importance to identify valuable transformations\")\n",
    "print(\"- Document all feature engineering steps for reproducibility\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
