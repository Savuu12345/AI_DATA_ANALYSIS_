{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question: Advanced Data Cleaning with Multiple Issues\n",
    "# Objective: Handle multiple issues in one dataset, including missing values, duplicates, and outliers\n",
    "# Description: Given a dataset with various data quality issues, employ multiple data cleaning techniques\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "def clean_data(df):\n",
    "    df = df.copy()\n",
    "    df = df.drop_duplicates()\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype in ['int64', 'float64']:\n",
    "            df[col].fillna(df[col].median(), inplace=True)\n",
    "        else:\n",
    "            df[col].fillna(df[col].mode()[0], inplace=True)\n",
    "    numeric_cols = df.select_dtypes(include=np.number).columns\n",
    "    for col in numeric_cols:\n",
    "        q1 = df[col].quantile(0.25)\n",
    "        q3 = df[col].quantile(0.75)\n",
    "        iqr = q3 - q1\n",
    "        lower_bound = q1 - 1.5 * iqr\n",
    "        upper_bound = q3 + 1.5 * iqr\n",
    "        df[col] = np.where(df[col] > upper_bound, upper_bound, \n",
    "                          np.where(df[col] < lower_bound, lower_bound, df[col]))\n",
    "    df = df.reset_index(drop=True)\n",
    "    return df\n",
    "data = {\n",
    "    'ID': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1],\n",
    "    'Age': [25, 32, 45, 28, np.nan, 25, 52, 38, 29, 150, 25],\n",
    "    'Income': [50000, 75000, 120000, 68000, 92000, 50000, 110000, 85000, np.nan, 2500000, 50000],\n",
    "    'Category': ['A', 'B', 'C', 'D', 'E', 'A', 'B', 'C', np.nan, 'F', 'A']\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "cleaned_df = clean_data(df)\n",
    "print(\"Original Data:\")\n",
    "print(df)\n",
    "print(\"\\nCleaned Data:\")\n",
    "print(cleaned_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question: Data Transformation Techniques\n",
    "# Objective: Transform skewed data using log transformation.\n",
    "# Description: Perform a log transformation to handle skewness in a dataset, which is particularly useful for\n",
    "# certain machine learning models\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "def log_transform(df, columns):\n",
    "    df = df.copy()\n",
    "    for col in columns:\n",
    "        if df[col].min() <= 0:\n",
    "            df[col] = np.log1p(df[col])\n",
    "        else:\n",
    "            df[col] = np.log(df[col])\n",
    "    return df\n",
    "def analyze_skewness(df, columns):\n",
    "    results = {}\n",
    "    for col in columns:\n",
    "        original_skew = stats.skew(df[col])\n",
    "        transformed = np.log1p(df[col]) if df[col].min() <= 0 else np.log(df[col])\n",
    "        transformed_skew = stats.skew(transformed)\n",
    "        results[col] = {\n",
    "            'original_skew': original_skew,\n",
    "            'transformed_skew': transformed_skew,\n",
    "            'improvement': original_skew - transformed_skew\n",
    "        }\n",
    "    return pd.DataFrame(results).T\n",
    "data = {\n",
    "    'Revenue': [1000, 1500, 2000, 2500, 3000, 3500, 4000, 4500, 5000, 100000],\n",
    "    'PageViews': [0, 50, 100, 150, 200, 250, 300, 350, 400, 10000],\n",
    "    'ConversionRate': [0.01, 0.02, 0.015, 0.03, 0.025, 0.02, 0.035, 0.04, 0.03, 0.5]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "numeric_cols = ['Revenue', 'PageViews', 'ConversionRate']\n",
    "print(\"Original Skewness:\")\n",
    "print(df[numeric_cols].skew())\n",
    "transformed_df = log_transform(df, numeric_cols)\n",
    "print(\"\\nTransformed Skewness:\")\n",
    "print(transformed_df[numeric_cols].skew())\n",
    "skewness_analysis = analyze_skewness(df, numeric_cols)\n",
    "print(\"\\nSkewness Improvement Analysis:\")\n",
    "print(skewness_analysis)\n",
    "plt.figure(figsize=(12, 6))\n",
    "for i, col in enumerate(numeric_cols, 1):\n",
    "    plt.subplot(2, 3, i)\n",
    "    plt.hist(df[col], bins=20)\n",
    "    plt.title(f'Original {col}')\n",
    "    plt.subplot(2, 3, i+3)\n",
    "    plt.hist(transformed_df[col], bins=20)\n",
    "    plt.title(f'Transformed {col}')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question: Feature Engineering by Creating New Features\n",
    "# Objective: Create a new feature based on existing features to add predictive power.\n",
    "# Description: Generate additional features from existing data to potentially improve the performance of\n",
    "# prediction models\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "california = fetch_california_housing()\n",
    "df = pd.DataFrame(california.data, columns=california.feature_names)\n",
    "df['PRICE'] = california.target\n",
    "def create_features(df):\n",
    "    df = df.copy()\n",
    "    df['ROOM_RATIO'] = df['AveRooms'] / df['HouseAge']\n",
    "    df['BEDRM_RATIO'] = df['AveBedrms'] / (df['AveRooms'] + 1e-6)\n",
    "    df['POP_INCOME_INTERACTION'] = df['Population'] * df['MedInc']\n",
    "    df['LAT_LONG_INTERACTION'] = df['Latitude'] * df['Longitude']\n",
    "    df['ROOM_INCOME_INTERACTION'] = df['AveRooms'] * df['MedInc']\n",
    "    df['HIGH_INCOME'] = (df['MedInc'] > df['MedInc'].median()).astype(int)\n",
    "    df['OLD_HOUSE'] = (df['HouseAge'] > df['HouseAge'].median()).astype(int)\n",
    "    return df\n",
    "engineered_df = create_features(df)\n",
    "\n",
    "print(\"Original Features:\")\n",
    "print(df.columns.tolist())\n",
    "print(\"\\nEngineered Features:\")\n",
    "print(set(engineered_df.columns) - set(df.columns))\n",
    "\n",
    "X_orig = df.drop('PRICE', axis=1)\n",
    "X_eng = engineered_df.drop('PRICE', axis=1)\n",
    "y = df['PRICE']\n",
    "\n",
    "X_train_orig, X_test_orig, y_train, y_test = train_test_split(X_orig, y, test_size=0.2, random_state=42)\n",
    "X_train_eng, X_test_eng, _, _ = train_test_split(X_eng, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model = RandomForestRegressor(random_state=42)\n",
    "model.fit(X_train_orig, y_train)\n",
    "orig_score = mean_squared_error(y_test, model.predict(X_test_orig))\n",
    "\n",
    "model.fit(X_train_eng, y_train)\n",
    "eng_score = mean_squared_error(y_test, model.predict(X_test_eng))\n",
    "\n",
    "print(f\"\\nOriginal Features MSE: {orig_score:.4f}\")\n",
    "print(f\"Engineered Features MSE: {eng_score:.4f}\")\n",
    "print(f\"Improvement: {(orig_score - eng_score)/orig_score*100:.2f}%\")\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(['Original', 'Engineered'], [orig_score, eng_score])\n",
    "plt.title('Model Performance Comparison')\n",
    "plt.ylabel('Mean Squared Error')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question: Handling Complex Outliers with Z-Scores\n",
    "# Objective: Detect and handle outliers using Z-score method.\n",
    "# Description: Use the Z-score method to identify outliers which significantly differ from the rest of the data points\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "def detect_outliers_zscore(df, columns, threshold=3):\n",
    "    df = df.copy()\n",
    "    outliers_mask = pd.DataFrame(index=df.index)\n",
    "    for col in columns:\n",
    "        z_scores = np.abs(stats.zscore(df[col]))\n",
    "        outliers_mask[col] = z_scores > threshold\n",
    "    return outliers_mask\n",
    "def handle_outliers(df, columns, method='cap', threshold=3):\n",
    "    df = df.copy()\n",
    "    outliers_info = {}\n",
    "    for col in columns:\n",
    "        z_scores = np.abs(stats.zscore(df[col]))\n",
    "        outliers = z_scores > threshold\n",
    "        outliers_info[col] = {\n",
    "            'count': sum(outliers),\n",
    "            'percentage': sum(outliers)/len(df)*100\n",
    "        }\n",
    "        if method == 'cap':\n",
    "            upper_bound = df[col].mean() + threshold * df[col].std()\n",
    "            lower_bound = df[col].mean() - threshold * df[col].std()\n",
    "            df[col] = np.where(df[col] > upper_bound, upper_bound,\n",
    "                              np.where(df[col] < lower_bound, lower_bound, df[col]))\n",
    "        elif method == 'remove':\n",
    "            df = df[z_scores <= threshold]\n",
    "    \n",
    "    return df, outliers_info\n",
    "np.random.seed(42)\n",
    "data = {\n",
    "    'Age': np.concatenate([np.random.normal(40, 15, 950), np.random.randint(100, 120, 50)]),\n",
    "    'Income': np.concatenate([np.random.normal(50000, 20000, 950), np.random.randint(200000, 500000, 50)]),\n",
    "    'Spending': np.concatenate([np.random.normal(1000, 500, 980), np.random.randint(5000, 10000, 20)])\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "numeric_cols = ['Age', 'Income', 'Spending']\n",
    "print(\"Original Data Summary:\")\n",
    "print(df.describe())\n",
    "outliers_mask = detect_outliers_zscore(df, numeric_cols)\n",
    "print(\"\\nOutliers Detected:\")\n",
    "print(outliers_mask.sum())\n",
    "df_capped, outliers_info = handle_outliers(df, numeric_cols, method='cap')\n",
    "print(\"\\nOutliers Information:\")\n",
    "for col, info in outliers_info.items():\n",
    "    print(f\"{col}: {info['count']} outliers ({info['percentage']:.2f}%)\")\n",
    "print(\"\\nProcessed Data Summary (Capped):\")\n",
    "print(df_capped.describe())\n",
    "plt.figure(figsize=(15, 5))\n",
    "for i, col in enumerate(numeric_cols, 1):\n",
    "    plt.subplot(1, 3, i)\n",
    "    plt.boxplot([df[col], df_capped[col]], labels=['Original', 'Processed'])\n",
    "    plt.title(col)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question: Data Imputation with K-Nearest Neighbors (KNN)\n",
    "# Objective: Impute missing numerical values using the KNN method.\n",
    "# Description: Use the K-nearest neighbors algorithm to fill in missing values, which considers the values of\n",
    "# nearest neighbors for imputation.\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "def knn_imputation(df, n_neighbors=5):\n",
    "    numeric_cols = df.select_dtypes(include=np.number).columns\n",
    "    scaler = StandardScaler()\n",
    "    scaled_data = scaler.fit_transform(df[numeric_cols])\n",
    "    \n",
    "    imputer = KNNImputer(n_neighbors=n_neighbors)\n",
    "    imputed_data = imputer.fit_transform(scaled_data)\n",
    "    \n",
    "    df_imputed = df.copy()\n",
    "    df_imputed[numeric_cols] = scaler.inverse_transform(imputed_data)\n",
    "    return df_imputed\n",
    "\n",
    "np.random.seed(42)\n",
    "data = {\n",
    "    'Age': np.random.normal(40, 15, 100),\n",
    "    'Income': np.random.normal(50000, 20000, 100),\n",
    "    'Spending': np.random.normal(1000, 500, 100)\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "missing_mask = np.random.random(df.shape) < 0.15\n",
    "df[missing_mask] = np.nan\n",
    "\n",
    "print(\"Original Data with Missing Values:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "df_imputed = knn_imputation(df)\n",
    "\n",
    "print(\"\\nAfter KNN Imputation:\")\n",
    "print(df_imputed.isnull().sum())\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "for i, col in enumerate(df.columns, 1):\n",
    "    plt.subplot(1, 3, i)\n",
    "    plt.hist(df[col], alpha=0.5, label='Original', bins=20)\n",
    "    plt.hist(df_imputed[col], alpha=0.5, label='Imputed', bins=20)\n",
    "    plt.title(col)\n",
    "    plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "comparison = pd.DataFrame({\n",
    "    'Original Mean': df.mean(),\n",
    "    'Imputed Mean': df_imputed.mean(),\n",
    "    'Original Std': df.std(),\n",
    "    'Imputed Std': df_imputed.std()\n",
    "})\n",
    "print(\"\\nStatistical Comparison:\")\n",
    "print(comparison)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
