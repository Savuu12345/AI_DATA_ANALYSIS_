{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: Automated Data Profiling\n",
    "\n",
    "**Steps**:\n",
    "1. Using Pandas-Profiling\n",
    "    - Generate a profile report for an existing CSV file.\n",
    "    - Customize the profile report to include correlations.\n",
    "    - Profile a specific subset of columns.\n",
    "2. Using Great Expectations\n",
    "    - Create a basic expectation suite for your data.\n",
    "    - Validate data against an expectation suite.\n",
    "    - Add multiple expectations to a suite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas_profiling'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Write your code from here\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# Write your code from here\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas_profiling\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ProfileReport\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgreat_expectations\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata_context\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DataContext\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtempfile\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pandas_profiling'"
     ]
    }
   ],
   "source": [
    "# Write your code from here\n",
    "# Write your code from here\n",
    "import pandas as pd\n",
    "from pandas_profiling import ProfileReport\n",
    "from great_expectations.data_context import DataContext\n",
    "import tempfile\n",
    "import os\n",
    "\n",
    "def profile_csv_with_pandas_profiling(file_path, subset_columns=None):\n",
    "    \"\"\"\n",
    "    Generates a Pandas-Profiling report for a CSV file.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Path to the CSV file.\n",
    "        subset_columns (list, optional): List of columns to profile. If None, profiles all.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "        if subset_columns:\n",
    "            try:\n",
    "                df = df[subset_columns]\n",
    "            except KeyError as e:\n",
    "                print(f\"Error: Column not found: {e}\")\n",
    "                return\n",
    "\n",
    "        profile = ProfileReport(df, title=\"Pandas Profiling Report\", correlations={\"pearson\": {\"calculate\": True}, \"spearman\": {\"calculate\": True}, \"kendall\": {\"calculate\": True}})\n",
    "        profile.to_file(\"pandas_profiling_report.html\")\n",
    "        print(\"Pandas Profiling report generated successfully at pandas_profiling_report.html\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File not found at {file_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "def create_gx_context():\n",
    "    \"\"\"\n",
    "    Creates a Great Expectations Data Context in a temporary directory.\n",
    "\n",
    "    Returns:\n",
    "        DataContext: The created Data Context.\n",
    "    \"\"\"\n",
    "    context_path = tempfile.mkdtemp()\n",
    "    context = DataContext.create(project_dir=context_path)\n",
    "    return context\n",
    "\n",
    "def create_expectation_suite(context, suite_name=\"my_expectation_suite\"):\n",
    "    \"\"\"\n",
    "    Creates a Great Expectations Expectation Suite.\n",
    "\n",
    "    Args:\n",
    "        context (DataContext): The Data Context to use.\n",
    "        suite_name (str, optional): The name of the Expectation Suite. Defaults to \"my_expectation_suite\".\n",
    "\n",
    "    Returns:\n",
    "        ExpectationSuite: The created Expectation Suite.\n",
    "    \"\"\"\n",
    "    suite = context.create_expectation_suite(\n",
    "        expectation_suite_name=suite_name, overwrite_existing=True\n",
    "    )\n",
    "    return suite\n",
    "\n",
    "def add_expectations(suite, df):\n",
    "    \"\"\"\n",
    "    Adds multiple expectations to a Great Expectations Expectation Suite based on the DataFrame.\n",
    "\n",
    "    Args:\n",
    "        suite (ExpectationSuite): The Expectation Suite to add expectations to.\n",
    "        df (pd.DataFrame): The DataFrame to infer expectations from.\n",
    "\n",
    "    Returns:\n",
    "        ExpectationSuite: The modified Expectation Suite.\n",
    "    \"\"\"\n",
    "    for column in df.columns:\n",
    "        # Expect the column to exist\n",
    "        suite.expect_column_to_exist(column=column)\n",
    "\n",
    "        # Try to infer the data type and add an appropriate expectation\n",
    "        if pd.api.types.is_numeric_dtype(df[column]):\n",
    "            min_val = df[column].min()\n",
    "            max_val = df[column].max()\n",
    "            suite.expect_column_values_to_be_between(\n",
    "                column=column, min_value=min_val, max_value=max_val\n",
    "            )\n",
    "        elif pd.api.types.is_datetime64_any_dtype(df[column]):\n",
    "            #Expect the dates to be in a reasonable range\n",
    "            min_date = pd.Timestamp('2000-01-01')\n",
    "            max_date = pd.Timestamp('2030-01-01')\n",
    "            suite.expect_column_values_to_be_between(\n",
    "                column=column, min_value=min_date, max_value=max_date\n",
    "            )\n",
    "        else:\n",
    "            unique_values = df[column].unique()\n",
    "            if len(unique_values) < 50:  # Limit to avoid very long lists\n",
    "                suite.expect_column_values_to_be_in_set(\n",
    "                    column=column, value_set=unique_values\n",
    "                )\n",
    "            else:\n",
    "                #For string columns with many unique values, expect non-nullity and a reasonable length\n",
    "                suite.expect_column_values_to_not_be_null(column=column)\n",
    "                suite.expect_column_value_lengths_to_be_between(column=column, min_value=1, max_value=255)\n",
    "    return suite\n",
    "\n",
    "def create_gx_datasource(context, df):\n",
    "    \"\"\"\n",
    "    Creates a Great Expectations Datasource from a Pandas DataFrame.\n",
    "\n",
    "    Args:\n",
    "        context (DataContext): The Great Expectations Data Context.\n",
    "        df (pd.DataFrame): The Pandas DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        BatchRequest: A BatchRequest object.\n",
    "        str: The path to the temporary CSV file.\n",
    "    \"\"\"\n",
    "    # Create a temporary CSV file\n",
    "    temp_file = tempfile.NamedTemporaryFile(delete=False, suffix=\".csv\")\n",
    "    temp_file_path = temp_file.name\n",
    "    df.to_csv(temp_file_path, index=False)\n",
    "    temp_file.close()\n",
    "\n",
    "    datasource_name = \"pandas_datasource\"\n",
    "    if datasource_name not in context.list_datasources():\n",
    "        context.add_pandas(name=datasource_name)\n",
    "\n",
    "    batch_request = context.make_pandas_data_asset(\n",
    "        file_path=temp_file_path,\n",
    "        asset_name=\"my_data_asset\",\n",
    "    ).build_batch_request()\n",
    "    return batch_request, temp_file_path\n",
    "\n",
    "def validate_data_with_gx(context, batch_request, suite):\n",
    "    \"\"\"\n",
    "    Validates data against a Great Expectations Expectation Suite.\n",
    "\n",
    "    Args:\n",
    "        context (DataContext): The Great Expectations Data Context.\n",
    "        batch_request (BatchRequest): The BatchRequest.\n",
    "        suite (ExpectationSuite): The Expectation Suite to validate against.\n",
    "\n",
    "    Returns:\n",
    "        dict: The validation results.\n",
    "    \"\"\"\n",
    "    results = context.run_validation_operator(\n",
    "        assets_to_validate=[batch_request],\n",
    "        expectation_suite_names=[suite.expectation_suite_name],\n",
    "    ).results\n",
    "    return results\n",
    "\n",
    "def display_gx_results(results):\n",
    "    \"\"\"\n",
    "    Displays the Great Expectations validation results.\n",
    "\n",
    "    Args:\n",
    "        results (dict): The validation results.\n",
    "    \"\"\"\n",
    "    if not results:\n",
    "        print(\"No validation results to display.\")\n",
    "        return\n",
    "\n",
    "    for result in results.values():\n",
    "        print(f\"Validation Results for batch: {result['batch_kwargs']}\")\n",
    "        for expectation_result in result[\"results\"]:\n",
    "            if not expectation_result[\"success\"]:\n",
    "                print(\n",
    "                    f\"  - Expectation '{expectation_result['expectation_config']['expectation_type']}' failed:\"\n",
    "                )\n",
    "                print(\n",
    "                    f\"    - Column: {expectation_result['expectation_config']['kwargs'].get('column', 'N/A')}\"\n",
    "                )\n",
    "                print(\n",
    "                    f\"    - Details: {expectation_result['result'].get('details', 'No details available')}\"\n",
    "                )\n",
    "        print(f\"  Summary: {result['success']}\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to run the data profiling and validation.\n",
    "    \"\"\"\n",
    "    # 1. Using Pandas-Profiling\n",
    "    file_path = \"data.csv\"\n",
    "    profile_csv_with_pandas_profiling(file_path)\n",
    "    profile_csv_with_pandas_profiling(file_path, subset_columns=[\"age\", \"income\"])\n",
    "\n",
    "    # 2. Using Great Expectations\n",
    "    context = create_gx_context()\n",
    "\n",
    "    # Load data with Pandas\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File not found at {file_path}.  Please make sure the file exists before running the script.\")\n",
    "        return  # Exit the program\n",
    "\n",
    "    suite = create_expectation_suite(context)\n",
    "    suite = add_expectations(suite, df) #Add expectations\n",
    "    context.save_expectation_suite(expectation_suite=suite) # save\n",
    "\n",
    "    batch_request, temp_file_path = create_gx_datasource(context, df)\n",
    "    results = validate_data_with_gx(context, batch_request, suite)\n",
    "    display_gx_results(results)\n",
    "\n",
    "    os.remove(temp_file_path) #Cleanup\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: Real-time Monitoring of Data Quality\n",
    "\n",
    "**Steps**:\n",
    "1. Setting up Alerts for Quality Drops\n",
    "    - Use the logging library to set up a basic alert on failed expectations.\n",
    "    - Implementing alerts using email notifications.\n",
    "    - Using a dashboard like Grafana for visual alerts.\n",
    "        - Note: Example assumes integration with a monitoring system\n",
    "        - Alert setup would involve creating a data source and alert rule in Grafana"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code from here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3: Using AI for Data Quality Monitoring\n",
    "**Steps**:\n",
    "1. Basic AI Models for Monitoring\n",
    "    - Train a simple anomaly detection model using Isolation Forest.\n",
    "    - Use a simple custom function based AI logic for outlier detection.\n",
    "    - Creating a monitoring function that utilizes a pre-trained machine learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code from here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
