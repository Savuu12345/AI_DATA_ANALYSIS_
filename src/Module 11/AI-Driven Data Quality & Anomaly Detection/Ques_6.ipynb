{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Deduplication using Clustering\n",
    "**Objective**: Learn and implement data deduplication techniques.\n",
    "\n",
    "**Task**: DBSCAN for Data Deduplication\n",
    "\n",
    "**Steps**:\n",
    "1. Data Set: Download a dataset containing duplicate entries for event registrations.\n",
    "2. DBSCAN Clustering: Apply the DBSCAN algorithm to cluster similar registrations.\n",
    "3. Identify Duplicates: Detect duplicates based on density of the clusters.\n",
    "4. Refinement: Validate clusters and remove any erroneous duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import DBSCAN\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.metrics import silhouette_score # Added for evaluation, though not directly for deduplication decision\n",
    "from scipy.spatial.distance import pdist, squareform # For density-based epsilon selection\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# --- 1. Data Set: Generate Synthetic Event Registration Data with Duplicates ---\n",
    "# More complex data generation to include names with potential typos\n",
    "def generate_registration_data_enhanced(num_records=1000, num_exact_dups=100, num_near_dups=50, num_outliers=10):\n",
    "    start_time = pd.to_datetime('2025-01-01 09:00:00')\n",
    "    time_range_seconds = (pd.to_datetime('2025-01-05 17:00:00') - start_time).total_seconds()\n",
    "\n",
    "    base_names = ['Alice Smith', 'Bob Johnson', 'Charlie Brown', 'Diana Prince', 'Eve Adams',\n",
    "                  'Frank White', 'Grace Taylor', 'Harry Wilson', 'Ivy King', 'Jack Green']\n",
    "    \n",
    "    data = {\n",
    "        'Registration_ID': range(num_records),\n",
    "        'Participant_Name': np.random.choice(base_names, num_records),\n",
    "        'Registration_Time_Seconds': np.random.uniform(0, time_range_seconds, num_records),\n",
    "        'Payment_Amount': np.random.uniform(50, 500, num_records).round(2),\n",
    "        'Tickets_Purchased': np.random.randint(1, 5, num_records),\n",
    "        'Event_Type': np.random.choice(['Conference', 'Workshop', 'Seminar', 'Webinar'], num_records)\n",
    "    }\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    df['Registration_Time'] = pd.to_datetime(start_time + pd.to_timedelta(df['Registration_Time_Seconds'], unit='s'))\n",
    "\n",
    "    # Introduce exact duplicates\n",
    "    exact_duplicate_rows = df.sample(n=num_exact_dups, replace=False, random_state=42).copy()\n",
    "    exact_duplicate_rows['Registration_ID'] = np.random.randint(num_records, num_records + num_exact_dups, num_exact_dups)\n",
    "    df = pd.concat([df, exact_duplicate_rows], ignore_index=True)\n",
    "\n",
    "    # Introduce near-duplicates (slight variations + name typos)\n",
    "    near_duplicate_rows = df.sample(n=num_near_dups, replace=False, random_state=100).copy()\n",
    "    near_duplicate_rows['Registration_ID'] = np.random.randint(num_records + num_exact_dups, num_records + num_exact_dups + num_near_dups, num_near_dups)\n",
    "    near_duplicate_rows['Payment_Amount'] = near_duplicate_rows['Payment_Amount'] + np.random.uniform(-5, 5, num_near_dups).round(2)\n",
    "    near_duplicate_rows['Registration_Time_Seconds'] = near_duplicate_rows['Registration_Time_Seconds'] + np.random.uniform(-3600, 3600, num_near_dups)\n",
    "    near_duplicate_rows['Tickets_Purchased'] = np.clip(near_duplicate_rows['Tickets_Purchased'] + np.random.randint(-1, 2, num_near_dups), 1, 5)\n",
    "    near_duplicate_rows['Registration_Time'] = pd.to_datetime(start_time + pd.to_timedelta(near_duplicate_rows['Registration_Time_Seconds'], unit='s'))\n",
    "\n",
    "    # Introduce typos in Participant_Name for near-duplicates\n",
    "    for i in range(len(near_duplicate_rows)):\n",
    "        name = list(near_duplicate_rows.iloc[i]['Participant_Name'])\n",
    "        if len(name) > 3 and np.random.rand() < 0.3: # 30% chance of typo\n",
    "            idx_to_change = np.random.randint(0, len(name))\n",
    "            name[idx_to_change] = chr(ord(name[idx_to_change]) + np.random.choice([-1, 1])) # Simple char change\n",
    "        near_duplicate_rows.iloc[i, near_duplicate_rows.columns.get_loc('Participant_Name')] = \"\".join(name)\n",
    "\n",
    "    df = pd.concat([df, near_duplicate_rows], ignore_index=True)\n",
    "\n",
    "    # Introduce some clear outliers\n",
    "    outlier_rows = df.sample(n=num_outliers, replace=False, random_state=200).copy()\n",
    "    outlier_rows['Registration_ID'] = np.random.randint(num_records + num_exact_dups + num_near_dups, num_records + num_exact_dups + num_near_dups + num_outliers, num_outliers)\n",
    "    outlier_rows['Payment_Amount'] = outlier_rows['Payment_Amount'] * np.random.uniform(5, 10, num_outliers)\n",
    "    outlier_rows['Registration_Time_Seconds'] = np.random.uniform(time_range_seconds * 1.5, time_range_seconds * 2, num_outliers)\n",
    "    outlier_rows['Registration_Time'] = pd.to_datetime(start_time + pd.to_timedelta(outlier_rows['Registration_Time_Seconds'], unit='s'))\n",
    "    df = pd.concat([df, outlier_rows], ignore_index=True)\n",
    "\n",
    "    df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "# Generate data\n",
    "registration_df = generate_registration_data_enhanced(num_records=700, num_exact_dups=70, num_near_dups=40, num_outliers=5)\n",
    "print(f\"Initial DataFrame shape: {registration_df.shape}\")\n",
    "print(\"Sample of initial DataFrame:\")\n",
    "print(registration_df.head())\n",
    "\n",
    "# Features for clustering (numerical features are key for DBSCAN direct application)\n",
    "# For 'Event_Type', if needed for clustering, one-hot encoding would be used.\n",
    "# For 'Participant_Name', string similarity (fuzzy matching) is usually applied POST-clustering\n",
    "# or via text embeddings if clustering directly on name similarity.\n",
    "numerical_features = ['Registration_Time_Seconds', 'Payment_Amount', 'Tickets_Purchased']\n",
    "X_numerical = registration_df[numerical_features]\n",
    "\n",
    "# --- 2. Preprocess: Standardize the data ---\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_numerical)\n",
    "X_scaled_df = pd.DataFrame(X_scaled, columns=numerical_features, index=registration_df.index)\n",
    "\n",
    "print(\"\\nSample of Scaled Data (used for DBSCAN):\")\n",
    "print(X_scaled_df.head())\n",
    "\n",
    "# --- 3. DBSCAN Clustering: Apply the algorithm ---\n",
    "\n",
    "# Determine min_samples: A common heuristic is 2 * number_of_features.\n",
    "# For noisy datasets or higher dimensions, values like 2*D or 2*D+1 are often used.\n",
    "min_samples_val = 2 * len(numerical_features)\n",
    "print(f\"\\nUsing min_samples = 2 * features = {min_samples_val}\")\n",
    "\n",
    "# Determine eps: Using the K-distance Graph (Elbow Method)\n",
    "# This plots the distance to the k-th (min_samples) nearest neighbor for each point.\n",
    "# An \"elbow\" indicates a good eps value.\n",
    "neigh = NearestNeighbors(n_neighbors=min_samples_val)\n",
    "nbrs = neigh.fit(X_scaled)\n",
    "distances, indices = nbrs.kneighbors(X_scaled)\n",
    "# Sort distances to the (min_samples-1)-th (k-th) neighbor\n",
    "distances = np.sort(distances[:, min_samples_val-1], axis=0)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(distances)\n",
    "plt.title(f'K-distance Graph for DBSCAN (k={min_samples_val} nearest neighbor)', fontsize=16)\n",
    "plt.xlabel('Points sorted by distance', fontsize=12)\n",
    "plt.ylabel(f'Distance to {min_samples_val}-th Nearest Neighbor', fontsize=12)\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Based on the elbow plot, manually choose an appropriate 'eps' value.\n",
    "# For deduplication, 'eps' needs to be very small to form tight clusters of duplicates.\n",
    "# A small value (e.g., 0.1 to 1.0 for scaled data) is typical.\n",
    "# You would pick the point right before the steep increase (the 'elbow').\n",
    "chosen_eps = 0.5 # **CRITICAL: ADJUST THIS VALUE BASED ON YOUR K-DISTANCE PLOT'S ELBOW**\n",
    "\n",
    "print(f\"\\nApplying DBSCAN with chosen eps={chosen_eps} and min_samples={min_samples_val}\")\n",
    "dbscan = DBSCAN(eps=chosen_eps, min_samples=min_samples_val)\n",
    "registration_df['Cluster'] = dbscan.fit_predict(X_scaled)\n",
    "\n",
    "print(\"\\nDataFrame with DBSCAN Cluster Assignments (first 10 rows):\")\n",
    "print(registration_df.head(10))\n",
    "print(f\"\\nCluster distribution (value counts, top 10):\")\n",
    "print(registration_df['Cluster'].value_counts().head(10))\n",
    "print(f\"Total number of clusters formed (excluding noise -1): {registration_df['Cluster'].nunique() - (1 if -1 in registration_df['Cluster'].values else 0)}\")\n",
    "print(f\"Number of noise points (cluster -1): {(registration_df['Cluster'] == -1).sum()}\")\n",
    "\n",
    "# --- 4. Identify Duplicates and Refinement ---\n",
    "# Records with Cluster = -1 are noise points (outliers) and are considered unique.\n",
    "# Records within clusters (Cluster >= 0) with more than one member are potential duplicate groups.\n",
    "\n",
    "deduplicated_records = []\n",
    "duplicates_removed_count = 0\n",
    "retained_original_indices = set() # To prevent adding the same row multiple times if indices overlap\n",
    "\n",
    "# Group by cluster ID\n",
    "grouped_by_cluster = registration_df.groupby('Cluster')\n",
    "\n",
    "for cluster_id, cluster_group in grouped_by_cluster:\n",
    "    if cluster_id == -1:\n",
    "        # Noise points are considered unique\n",
    "        for idx, row in cluster_group.iterrows():\n",
    "            if idx not in retained_original_indices:\n",
    "                deduplicated_records.append(row)\n",
    "                retained_original_indices.add(idx)\n",
    "    else:\n",
    "        # For non-noise clusters, identify duplicates within them\n",
    "        if len(cluster_group) > 1: # Only proceed if the cluster has multiple members\n",
    "            # For numerical features, group by exact matches to confirm duplicates.\n",
    "            # For 'Participant_Name', one might apply fuzzy string matching here\n",
    "            # (e.g., check Jaro-Winkler similarity > 0.85).\n",
    "            \n",
    "            # Create a hashable tuple of the numerical features for grouping exact duplicates\n",
    "            cluster_group['features_tuple'] = list(zip(*[cluster_group[col] for col in numerical_features]))\n",
    "            \n",
    "            # Group by this hashable tuple to find truly identical numerical records within the cluster\n",
    "            sub_grouped_by_exact_features = cluster_group.groupby('features_tuple')\n",
    "\n",
    "            for _, sub_group in sub_grouped_by_exact_features:\n",
    "                if len(sub_group) > 1:\n",
    "                    # These are multiple records in the same exact numerical feature space within this cluster.\n",
    "                    # They are strong candidates for duplicates.\n",
    "                    duplicates_removed_count += (len(sub_group) - 1)\n",
    "                    # Keep the first record of this exact group as the representative\n",
    "                    if sub_group.iloc[0].name not in retained_original_indices:\n",
    "                        deduplicated_records.append(sub_group.iloc[0])\n",
    "                        retained_original_indices.add(sub_group.iloc[0].name)\n",
    "                else:\n",
    "                    # This record is numerically unique within its cluster, but still part of a dense cluster.\n",
    "                    # It's kept as a unique entity.\n",
    "                    if sub_group.iloc[0].name not in retained_original_indices:\n",
    "                        deduplicated_records.append(sub_group.iloc[0])\n",
    "                        retained_original_indices.add(sub_group.iloc[0].name)\n",
    "        else:\n",
    "            # Cluster has only one member (it's unique by DBSCAN's density definition)\n",
    "            if cluster_group.iloc[0].name not in retained_original_indices:\n",
    "                deduplicated_records.append(cluster_group.iloc[0])\n",
    "                retained_original_indices.add(cluster_group.iloc[0].name)\n",
    "\n",
    "# Create the final deduplicated DataFrame\n",
    "# Drop the temporary 'Cluster' and 'features_tuple' columns\n",
    "final_df = pd.DataFrame(deduplicated_records).drop(columns=['Cluster', 'features_tuple'], errors='ignore')\n",
    "\n",
    "print(f\"\\nTotal potential duplicates identified and removed: {duplicates_removed_count}\")\n",
    "print(f\"Original DataFrame size: {len(registration_df)}\")\n",
    "print(f\"Deduplicated DataFrame size: {len(final_df)}\")\n",
    "print(\"\\nSample of Deduplicated DataFrame:\")\n",
    "print(final_df.head())\n",
    "\n",
    "# --- Verification ---\n",
    "print(\"\\n--- Verification of Deduplication Impact ---\")\n",
    "# Check exact duplicates based on numerical features before and after\n",
    "num_exact_dups_before = registration_df.duplicated(subset=numerical_features).sum()\n",
    "print(f\"Number of exact duplicates (on numerical features) before DBSCAN processing: {num_exact_dups_before}\")\n",
    "\n",
    "num_exact_dups_after = final_df.duplicated(subset=numerical_features).sum()\n",
    "print(f\"Number of exact duplicates (on numerical features) after DBSCAN deduplication: {num_exact_dups_after}\")\n",
    "\n",
    "# Compare total unique records\n",
    "num_unique_records_original = registration_df.drop_duplicates(subset=numerical_features).shape[0]\n",
    "print(f\"Number of truly unique records (numerical features) in original DF: {num_unique_records_original}\")\n",
    "print(f\"Number of records in final (deduplicated) DF: {len(final_df)}\")\n",
    "\n",
    "if num_exact_dups_after == 0:\n",
    "    print(\"\\nSuccess: No exact duplicates (on numerical features) remaining in the deduplicated DataFrame.\")\n",
    "else:\n",
    "    print(\"\\nWarning: Some exact duplicates (on numerical features) might still remain. Adjust DBSCAN 'eps'/'min_samples' or refine intra-cluster deduplication logic.\")\n",
    "\n",
    "# Optional: Visualize clusters (if 2D or 3D)\n",
    "# If you have more than 2-3 numerical features, direct scatter plot isn't helpful.\n",
    "# Consider using PCA/t-SNE for dimensionality reduction before visualizing high-dim data.\n",
    "if len(numerical_features) >= 2:\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    # Plotting only the first two scaled features for visualization\n",
    "    sns.scatterplot(x=X_scaled[:, 0], y=X_scaled[:, 1], hue=registration_df['Cluster'],\n",
    "                    palette='viridis', legend='full', s=50, alpha=0.7)\n",
    "    plt.title('DBSCAN Clusters of Registration Data (Scaled Features)', fontsize=16)\n",
    "    plt.xlabel(f'Scaled {numerical_features[0]}', fontsize=12)\n",
    "    plt.ylabel(f'Scaled {numerical_features[1]}', fontsize=12)\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
