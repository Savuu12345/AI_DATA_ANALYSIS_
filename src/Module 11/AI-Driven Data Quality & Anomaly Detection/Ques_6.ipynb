{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Deduplication using Clustering\n",
    "**Objective**: Learn and implement data deduplication techniques.\n",
    "\n",
    "**Task**: DBSCAN for Data Deduplication\n",
    "\n",
    "**Steps**:\n",
    "1. Data Set: Download a dataset containing duplicate entries for event registrations.\n",
    "2. DBSCAN Clustering: Apply the DBSCAN algorithm to cluster similar registrations.\n",
    "3. Identify Duplicates: Detect duplicates based on density of the clusters.\n",
    "4. Refinement: Validate clusters and remove any erroneous duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import DBSCAN\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import time\n",
    "\n",
    "\n",
    "np.random.seed(42)\n",
    "def generate_registration_data(num_records=1000,num_exact_dups=100,num_near_dups=50,num_outliers=10):\n",
    "    start_time=pd.to_datetime('2025-01-01 09:00:00')\n",
    "    end_time=pd.to_datetime('2025-01-05 17:00:00')\n",
    "    time_range_seconds=(end_time-start_time).total_seconds()\n",
    "    data={\n",
    "        'Registration_ID':range(num_records),\n",
    "        'Participant_Name':[f'Person_{i}' for i in np.random.randint(1,num_records//2,num_records)],\n",
    "        'Registration_Time_Seconds':np.random.uniform(0,time_range_seconds,num_records),\n",
    "        'Payment_Amount':np.random.uniform(50,500,num_records).round(2),\n",
    "        'Tickets_Purchased':np.random.randint(1,5,num_records),\n",
    "        'Event_Type':np.random.choice(['Conference','Workshop','Seminar','Webinar'],num_records)\n",
    "    }\n",
    "    df=pd.DataFrame(data)\n",
    "    df['Registration_Time']=pd.to_datetime(start_time+pd.to_timedelta(df['Registration_Time_Seconds'],unit='s'))\n",
    "    exact_duplicate_rows=df.sample(n=num_exact_dups,replace=False,random_state=42).copy()\n",
    "    exact_duplicate_rows['Registration_ID']=np.random.randint(num_records,num_records+num_exact_dups,num_exact_dups)\n",
    "    df=pd.concat([df,exact_duplicate_rows],ignore_index=True)\n",
    "    near_duplicate_rows=df.sample(n=num_near_dups,replace=False,random_state=100).copy()\n",
    "    near_duplicate_rows['Registration_ID']=np.random.randint(num_records+num_exact_dups,num_records+num_exact_dups+num_near_dups,num_near_dups)\n",
    "    near_duplicate_rows['Payment_Amount']=near_duplicate_rows['Payment_Amount']+np.random.uniform(-5,5,num_near_dups).round(2)\n",
    "    near_duplicate_rows['Registration_Time_Seconds']=near_duplicate_rows['Registration_Time_Seconds']+np.random.uniform(-3600,3600,num_near_dups)\n",
    "    near_duplicate_rows['Tickets_Purchased']=near_duplicate_rows['Tickets_Purchased']+np.random.randint(-1,2,num_near_dups)\n",
    "    near_duplicate_rows['Tickets_Purchased']=np.clip(near_duplicate_rows['Tickets_Purchased'],1,5)\n",
    "    near_duplicate_rows['Registration_Time']=pd.to_datetime(start_time+pd.to_timedelta(near_duplicate_rows['Registration_Time_Seconds'],unit='s'))\n",
    "    df=pd.concat([df,near_duplicate_rows],ignore_index=True)\n",
    "    outlier_rows=df.sample(n=num_outliers,replace=False,random_state=200).copy()\n",
    "    outlier_rows['Registration_ID']=np.random.randint(num_records+num_exact_dups+num_near_dups,num_records+num_exact_dups+num_near_dups+num_outliers,num_outliers)\n",
    "    outlier_rows['Payment_Amount']=outlier_rows['Payment_Amount']*np.random.uniform(5,10,num_outliers)\n",
    "    outlier_rows['Registration_Time_Seconds']=np.random.uniform(time_range_seconds*1.5,time_range_seconds*2,num_outliers)\n",
    "    outlier_rows['Registration_Time']=pd.to_datetime(start_time+pd.to_timedelta(outlier_rows['Registration_Time_Seconds'],unit='s'))\n",
    "    df=pd.concat([df,outlier_rows],ignore_index=True)\n",
    "    df=df.sample(frac=1,random_state=42).reset_index(drop=True)\n",
    "    return df\n",
    "registration_df=generate_registration_data(num_records=700,num_exact_dups=70,num_near_dups=40,num_outliers=5)\n",
    "print(f\"Initial DataFrame shape: {registration_df.shape}\")\n",
    "print(\"Sample of initial DataFrame:\")\n",
    "print(registration_df.head())\n",
    "numerical_features=['Registration_Time_Seconds','Payment_Amount','Tickets_Purchased']\n",
    "print(\"Numerical features to be used for clustering:\")\n",
    "print(numerical_features)\n",
    "X_numerical=registration_df[numerical_features]\n",
    "scaler=StandardScaler()\n",
    "X_scaled=scaler.fit_transform(X_numerical)\n",
    "X_scaled_df=pd.DataFrame(X_scaled,columns=numerical_features,index=registration_df.index)\n",
    "print(\"\\nSample of Scaled Data (used for DBSCAN):\")\n",
    "print(X_scaled_df.head())\n",
    "min_samples_val=2*len(numerical_features)\n",
    "neigh=NearestNeighbors(n_neighbors=min_samples_val)\n",
    "nbrs=neigh.fit(X_scaled)\n",
    "distances,indices=nbrs.kneighbors(X_scaled)\n",
    "distances=np.sort(distances[:,min_samples_val-1],axis=0)\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(distances)\n",
    "plt.title(f'K-distance Graph (k={min_samples_val})')\n",
    "plt.xlabel('Points sorted by distance')\n",
    "plt.ylabel(f'Distance to {min_samples_val}-th Nearest Neighbor')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "chosen_eps=0.5\n",
    "print(f\"\\nApplying DBSCAN with eps={chosen_eps} and min_samples={min_samples_val}\")\n",
    "dbscan=DBSCAN(eps=chosen_eps,min_samples=min_samples_val)\n",
    "registration_df['Cluster']=dbscan.fit_predict(X_scaled)\n",
    "print(\"\\nDataFrame with DBSCAN Cluster Assignments:\")\n",
    "print(registration_df.head())\n",
    "print(f\"\\nNumber of records per cluster:\\n{registration_df['Cluster'].value_counts().sort_index()}\")\n",
    "print(f\"Total number of clusters formed (excluding noise -1): {registration_df['Cluster'].nunique()-(1 if -1 in registration_df['Cluster'].values else 0)}\")\n",
    "print(f\"Number of noise points (cluster -1): {(registration_df['Cluster']==-1).sum()}\")\n",
    "deduplicated_records=[]\n",
    "duplicates_removed_count=0\n",
    "retained_indices=set()\n",
    "for cluster_id in sorted(registration_df['Cluster'].unique()):\n",
    "    cluster_group=registration_df[registration_df['Cluster']==cluster_id].copy()\n",
    "    if cluster_id==-1:\n",
    "        for index,row in cluster_group.iterrows():\n",
    "            if index not in retained_indices:\n",
    "                deduplicated_records.append(row)\n",
    "                retained_indices.add(index)\n",
    "    else:\n",
    "        if len(cluster_group)>1:\n",
    "            cluster_group['features_hashable']=list(zip(*[cluster_group[col] for col in numerical_features]))\n",
    "            sub_grouped_by_exact_features=cluster_group.groupby('features_hashable')\n",
    "            for _,sub_group in sub_grouped_by_exact_features:\n",
    "                if len(sub_group)>1:\n",
    "                    duplicates_removed_count+=(len(sub_group)-1)\n",
    "                    if sub_group.iloc[0].name not in retained_indices:\n",
    "                        deduplicated_records.append(sub_group.iloc[0])\n",
    "                        retained_indices.add(sub_group.iloc[0].name)\n",
    "                else:\n",
    "                    if sub_group.iloc[0].name not in retained_indices:\n",
    "                        deduplicated_records.append(sub_group.iloc[0])\n",
    "                        retained_indices.add(sub_group.iloc[0].name)\n",
    "        else:\n",
    "            if cluster_group.iloc[0].name not in retained_indices:\n",
    "                deduplicated_records.append(cluster_group.iloc[0])\n",
    "                retained_indices.add(cluster_group.iloc[0].name)\n",
    "final_df=pd.DataFrame(deduplicated_records).drop(columns=['Cluster','features_hashable'],errors='ignore')\n",
    "print(f\"\\nTotal potential duplicates identified and removed: {duplicates_removed_count}\")\n",
    "print(f\"Original DataFrame size: {len(registration_df)}\")\n",
    "print(f\"Deduplicated DataFrame size: {len(final_df)}\")\n",
    "print(\"\\nSample of Deduplicated DataFrame:\")\n",
    "print(final_df.head())\n",
    "print(\"\\n--- Verification of Deduplication Impact ---\")\n",
    "num_exact_dups_before=registration_df.duplicated(subset=numerical_features).sum()\n",
    "print(f\"Number of exact duplicates (on numerical features) before DBSCAN processing: {num_exact_dups_before}\")\n",
    "num_exact_dups_after=final_df.duplicated(subset=numerical_features).sum()\n",
    "print(f\"Number of exact duplicates (on numerical features) after DBSCAN deduplication: {num_exact_dups_after}\")\n",
    "num_unique_records_original=registration_df.drop_duplicates(subset=numerical_features).shape[0]\n",
    "print(f\"Number of truly unique records (numerical features) in original DF: {num_unique_records_original}\")\n",
    "print(f\"Number of records in final (deduplicated) DF: {len(final_df)}\")\n",
    "if num_exact_dups_after==0:\n",
    "    print(\"\\nSuccess: No exact duplicates (on numerical features) remaining in the deduplicated DataFrame.\")\n",
    "else:\n",
    "    print(\"\\nWarning: Some exact duplicates (on numerical features) might still remain. Adjust DBSCAN 'eps'/'min_samples' or refine intra-cluster deduplication logic.\")\n",
    "if len(numerical_features)>=2:\n",
    "    plt.figure(figsize=(12,8))\n",
    "    sns.scatterplot(x=X_scaled[:,0],y=X_scaled[:,1],hue=registration_df['Cluster'],palette='viridis',legend='full',s=50,alpha=0.7)\n",
    "    plt.title('DBSCAN Clusters of Registration Data (Scaled Features)',fontsize=16)\n",
    "    plt.xlabel(f'Scaled {numerical_features[0]}',fontsize=12)\n",
    "    plt.ylabel(f'Scaled {numerical_features[1]}',fontsize=12)\n",
    "    plt.grid(True)\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
