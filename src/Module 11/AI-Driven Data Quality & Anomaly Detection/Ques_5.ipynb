{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Deduplication using Clustering\n",
    "**Objective**: Learn and implement data deduplication techniques.\n",
    "\n",
    "**Task**: Hierarchical Clustering for Deduplication\n",
    "\n",
    "**Steps**:\n",
    "1. Data Set: Obtain a dataset containing duplicate employee information.\n",
    "2. Perform Clustering: Use hierarchical agglomerative clustering to cluster the employee\n",
    "records.\n",
    "3. Evaluate Duplicates: Determine duplicates by analyzing the clusters formed.\n",
    "4. Clean Data: Remove duplicate employee records found during clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your code from here\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# --- 1. Data Set: Generate Synthetic Employee Information with Duplicates ---\n",
    "def generate_employee_data(num_employees=500, num_exact_dups=50, num_near_dups=30):\n",
    "    data = {\n",
    "        'Employee_ID': range(num_employees),\n",
    "        'Age': np.random.randint(22, 60, num_employees),\n",
    "        'Years_Experience': np.random.randint(0, 30, num_employees),\n",
    "        'Salary': np.random.randint(40000, 120000, num_employees),\n",
    "        'Project_Count': np.random.randint(0, 10, num_employees),\n",
    "        'Department': np.random.choice(['HR', 'Engineering', 'Sales', 'Marketing', 'Finance'], num_employees),\n",
    "        'Name_First': np.random.choice(['John', 'Jane', 'Michael', 'Emily', 'Chris', 'Sarah', 'David', 'Laura'], num_employees),\n",
    "        'Name_Last': np.random.choice(['Doe', 'Smith', 'Jones', 'Williams', 'Brown', 'Davis', 'Miller', 'Wilson'], num_employees)\n",
    "    }\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # Combine first and last names for a full name feature\n",
    "    df['Full_Name'] = df['Name_First'] + ' ' + df['Name_Last']\n",
    "\n",
    "    # Introduce exact duplicates\n",
    "    exact_duplicate_rows = df.sample(n=num_exact_dups, replace=False, random_state=42).copy()\n",
    "    exact_duplicate_rows['Employee_ID'] = np.random.randint(num_employees, num_employees + num_exact_dups, num_exact_dups)\n",
    "    df = pd.concat([df, exact_duplicate_rows], ignore_index=True)\n",
    "\n",
    "    # Introduce near-duplicates (slight variations in numerical features)\n",
    "    near_duplicate_rows = df.sample(n=num_near_dups, replace=False, random_state=100).copy()\n",
    "    near_duplicate_rows['Employee_ID'] = np.random.randint(num_employees + num_exact_dups, num_employees + num_exact_dups + num_near_dups, num_near_dups)\n",
    "    near_duplicate_rows['Salary'] = near_duplicate_rows['Salary'] + np.random.randint(-500, 501, num_near_dups) # small salary variation\n",
    "    near_duplicate_rows['Years_Experience'] = near_duplicate_rows['Years_Experience'] + np.random.randint(-1, 2, num_near_dups) # +/- 1 year\n",
    "    near_duplicate_rows['Project_Count'] = near_duplicate_rows['Project_Count'] + np.random.randint(0, 2, num_near_dups) # +0 or +1 project\n",
    "    # For some near-duplicates, introduce typos in names (simple example)\n",
    "    for i in range(len(near_duplicate_rows)):\n",
    "        if np.random.rand() < 0.2: # 20% chance to slightly alter first name\n",
    "            name = list(near_duplicate_rows.iloc[i, near_duplicate_rows.columns.get_loc('Name_First')])\n",
    "            if len(name) > 2:\n",
    "                idx_to_change = np.random.randint(1, len(name) - 1)\n",
    "                name[idx_to_change] = chr(ord(name[idx_to_change]) + np.random.choice([-1, 1]))\n",
    "                near_duplicate_rows.iloc[i, near_duplicate_rows.columns.get_loc('Name_First')] = \"\".join(name)\n",
    "    near_duplicate_rows['Full_Name'] = near_duplicate_rows['Name_First'] + ' ' + near_duplicate_rows['Name_Last']\n",
    "    df = pd.concat([df, near_duplicate_rows], ignore_index=True)\n",
    "\n",
    "    # Shuffle the DataFrame\n",
    "    df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "employee_df = generate_employee_data(num_employees=500, num_exact_dups=50, num_near_dups=30)\n",
    "print(f\"Initial DataFrame shape: {employee_df.shape}\")\n",
    "print(\"Sample of initial DataFrame:\")\n",
    "print(employee_df.head())\n",
    "\n",
    "# Features for clustering (numerical features)\n",
    "# Note: For real-world names/text, you'd typically use fuzzy matching or text embeddings\n",
    "# after initial clustering or as part of a more sophisticated blocking strategy.\n",
    "numerical_features = ['Age', 'Years_Experience', 'Salary', 'Project_Count']\n",
    "X_numerical = employee_df[numerical_features]\n",
    "\n",
    "# --- 2. Preprocess: Standardize the numerical data ---\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_numerical)\n",
    "X_scaled_df = pd.DataFrame(X_scaled, columns=numerical_features, index=employee_df.index)\n",
    "\n",
    "print(\"\\nSample of Scaled Numerical Data:\")\n",
    "print(X_scaled_df.head())\n",
    "\n",
    "# --- 3. Perform Clustering: Hierarchical Agglomerative Clustering ---\n",
    "\n",
    "# Calculate the linkage matrix for the dendrogram\n",
    "# 'ward' linkage minimizes the variance of the clusters being merged.\n",
    "linked = linkage(X_scaled, method='ward')\n",
    "\n",
    "# Plot the dendrogram to visualize clusters and help determine distance threshold\n",
    "plt.figure(figsize=(20, 10))\n",
    "dendrogram(linked,\n",
    "           orientation='top',\n",
    "           labels=employee_df.index.tolist(), # Use DataFrame indices as labels\n",
    "           distance_sort='descending',\n",
    "           show_leaf_counts=True,\n",
    "           leaf_rotation=90,\n",
    "           leaf_font_size=8)\n",
    "plt.title('Hierarchical Clustering Dendrogram for Employee Data', fontsize=18)\n",
    "plt.xlabel('Data Point Index (from original DataFrame)', fontsize=14)\n",
    "plt.ylabel('Euclidean Distance', fontsize=14)\n",
    "# Add a horizontal line to suggest a cut-off threshold\n",
    "# This threshold is chosen visually from the dendrogram;\n",
    "# a smaller threshold means tighter clusters (more likely to be duplicates)\n",
    "suggested_threshold = 5 # Adjust based on the dendrogram visually\n",
    "plt.axhline(y=suggested_threshold, color='r', linestyle='--', label=f'Suggested Cut-off: {suggested_threshold}')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Apply Agglomerative Clustering with a chosen distance threshold\n",
    "# Records closer than this threshold will be in the same cluster\n",
    "# Choose a distance_threshold based on dendrogram or domain knowledge for deduplication\n",
    "# A very small threshold will result in many small clusters, ideal for finding duplicates.\n",
    "# For this synthetic data, a threshold around 2-5 might work well for very close duplicates.\n",
    "clustering_threshold = 2.0 # Experiment with this value based on dendrogram and desired strictness\n",
    "\n",
    "print(f\"\\nApplying Agglomerative Clustering with distance_threshold = {clustering_threshold}\")\n",
    "agg_cluster = AgglomerativeClustering(n_clusters=None, # None means distance_threshold must be set\n",
    "                                      distance_threshold=clustering_threshold,\n",
    "                                      linkage='ward') # 'ward' linkage for this example\n",
    "\n",
    "employee_df['Cluster'] = agg_cluster.fit_predict(X_scaled)\n",
    "\n",
    "print(\"\\nDataFrame with Cluster Assignments:\")\n",
    "print(employee_df.head())\n",
    "print(f\"\\nNumber of records per cluster (top 10 largest):\\n{employee_df['Cluster'].value_counts().head(10)}\")\n",
    "print(f\"Total number of clusters formed: {employee_df['Cluster'].nunique()}\")\n",
    "\n",
    "# --- 4. Evaluate and Clean Data: Identify and Remove Duplicates within Clusters ---\n",
    "\n",
    "deduplicated_records = []\n",
    "duplicates_removed_count = 0\n",
    "\n",
    "# Group by cluster\n",
    "grouped_by_cluster = employee_df.groupby('Cluster')\n",
    "\n",
    "for cluster_id, cluster_group in grouped_by_cluster:\n",
    "    if len(cluster_group) == 1:\n",
    "        # If a cluster has only one member, it's unique by definition (at this threshold)\n",
    "        deduplicated_records.append(cluster_group.iloc[0])\n",
    "    else:\n",
    "        # If a cluster has multiple members, these are potential duplicates/near-duplicates\n",
    "        # We need a strategy to select one representative and discard others.\n",
    "        # For this example, we'll use exact match on numerical features within the cluster\n",
    "        # and then select the first occurrence of each unique combination.\n",
    "        # In a real scenario, you'd apply fuzzy matching or a specific tie-breaking rule.\n",
    "\n",
    "        # Convert numerical features to a hashable tuple for grouping\n",
    "        cluster_group['features_tuple'] = list(zip(*[cluster_group[col] for col in numerical_features]))\n",
    "        \n",
    "        # Group by the exact feature values within this sub-cluster\n",
    "        sub_grouped = cluster_group.groupby('features_tuple')\n",
    "\n",
    "        for _, sub_group in sub_grouped:\n",
    "            if len(sub_group) > 1:\n",
    "                # These are multiple records in the same exact numerical feature space within a cluster\n",
    "                duplicates_removed_count += (len(sub_group) - 1)\n",
    "                # Keep the first record as the representative\n",
    "                deduplicated_records.append(sub_group.iloc[0])\n",
    "            else:\n",
    "                # Unique record within this exact numerical feature space\n",
    "                deduplicated_records.append(sub_group.iloc[0])\n",
    "\n",
    "# Create the final deduplicated DataFrame\n",
    "final_df = pd.DataFrame(deduplicated_records).drop(columns=['Cluster', 'features_tuple'], errors='ignore') # 'errors=ignore' for 'features_tuple' if not all groups create it\n",
    "\n",
    "print(f\"\\nTotal duplicates removed: {duplicates_removed_count}\")\n",
    "print(f\"Original DataFrame size: {len(employee_df)}\")\n",
    "print(f\"Deduplicated DataFrame size: {len(final_df)}\")\n",
    "print(\"\\nSample of Deduplicated DataFrame:\")\n",
    "print(final_df.head())\n",
    "\n",
    "# --- Verification ---\n",
    "print(\"\\n--- Verification of Deduplication Impact ---\")\n",
    "# Check exact duplicates based on numerical features before and after\n",
    "num_exact_dups_before_processing = employee_df.duplicated(subset=numerical_features).sum()\n",
    "print(f\"Number of exact duplicates (based on numerical features) before processing: {num_exact_dups_before_processing}\")\n",
    "\n",
    "num_exact_dups_after_processing = final_df.duplicated(subset=numerical_features).sum()\n",
    "print(f\"Number of exact duplicates (based on numerical features) after hierarchical deduplication: {num_exact_dups_after_processing}\")\n",
    "\n",
    "# Compare unique records in the original vs. final DataFrame based on numerical features\n",
    "num_unique_records_original = employee_df.drop_duplicates(subset=numerical_features).shape[0]\n",
    "print(f\"Number of truly unique records (numerical features) in original DF: {num_unique_records_original}\")\n",
    "print(f\"Number of records in final (deduplicated) DF: {len(final_df)}\")\n",
    "\n",
    "if num_exact_dups_after_processing == 0:\n",
    "    print(\"\\nSuccess: No exact duplicates (based on numerical features) remaining in the deduplicated DataFrame.\")\n",
    "else:\n",
    "    print(\"\\nWarning: Some exact duplicates (based on numerical features) might still remain. Adjust 'clustering_threshold' or refine intra-cluster deduplication logic.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
