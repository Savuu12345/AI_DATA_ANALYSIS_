{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AI & Machine Learning for Data Quality\n",
    "**Description**: AI and machine learning can automate and enhance data quality checks by learning patterns and identifying anomalies more effectively than static rules.\n",
    "\n",
    "**Task 1**: Training a model to predict and flag unusual trend patterns in sales data that\n",
    "deviate from historical norms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your code from here\n",
    "# write your code from here\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "def generate_sample_data(n_samples=730, anomaly_percentage=0.05):\n",
    "    \"\"\"\n",
    "    Generate sample sales data with some anomalies\n",
    "    n_samples: Number of days (2 years by default)\n",
    "    anomaly_percentage: Percentage of anomalies to introduce\n",
    "    \"\"\"\n",
    "    # Set seed for reproducibility\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Create date range for 2 years\n",
    "    start_date = datetime(2023, 1, 1)\n",
    "    dates = [start_date + timedelta(days=i) for i in range(n_samples)]\n",
    "    \n",
    "    # Base sales pattern with weekly seasonality and general upward trend\n",
    "    weekday_effect = np.array([0.8, 1.0, 1.1, 1.2, 1.3, 1.5, 0.9])  # Mon-Sun\n",
    "    weekday_indices = [d.weekday() for d in dates]\n",
    "    \n",
    "    # Create seasonal components\n",
    "    day_of_year = np.array([d.timetuple().tm_yday for d in dates])\n",
    "    yearly_cycle = 0.3 * np.sin(2 * np.pi * day_of_year / 365) + 0.2  # Yearly cycle\n",
    "    \n",
    "    # Trend component (gradual growth)\n",
    "    trend = np.linspace(0, 0.5, n_samples)\n",
    "    \n",
    "    # Base sales with seasonality and trend\n",
    "    base_sales = 100 + 20 * np.array([weekday_effect[idx] for idx in weekday_indices]) + 15 * yearly_cycle + 30 * trend\n",
    "    \n",
    "    # Add random noise\n",
    "    noise = np.random.normal(0, 5, n_samples)\n",
    "    sales = base_sales + noise\n",
    "    \n",
    "    # Add some special events (peaks)\n",
    "    # Black Friday effects\n",
    "    black_friday_indices = []\n",
    "    for year in [2023, 2024]:\n",
    "        # Approximate Black Friday as Nov 25\n",
    "        bf_idx = (datetime(year, 11, 25) - start_date).days\n",
    "        if 0 <= bf_idx < n_samples:\n",
    "            black_friday_indices.append(bf_idx)\n",
    "            sales[bf_idx] *= 2.5  # Major sales spike\n",
    "            # Days before and after also affected\n",
    "            for i in range(1, 4):\n",
    "                if bf_idx-i >= 0:\n",
    "                    sales[bf_idx-i] *= (1.2 + 0.1*i)  # Ramp up\n",
    "                if bf_idx+i < n_samples:\n",
    "                    sales[bf_idx+i] *= (1.3 - 0.1*i)  # Ramp down\n",
    "    \n",
    "    # Add holiday effects (Christmas season)\n",
    "    for year in [2023, 2024]:\n",
    "        christmas_start = (datetime(year, 12, 10) - start_date).days\n",
    "        christmas_end = (datetime(year, 12, 26) - start_date).days\n",
    "        if christmas_start >= 0 and christmas_start < n_samples:\n",
    "            end_idx = min(christmas_end, n_samples-1)\n",
    "            sales[christmas_start:end_idx] *= 1.8\n",
    "    \n",
    "    # Introduce anomalies\n",
    "    n_anomalies = int(n_samples * anomaly_percentage)\n",
    "    anomaly_indices = np.random.choice(range(n_samples), size=n_anomalies, replace=False)\n",
    "    \n",
    "    # Different types of anomalies\n",
    "    for idx in anomaly_indices:\n",
    "        anomaly_type = np.random.choice(['drop', 'spike', 'level_shift'])\n",
    "        \n",
    "        if anomaly_type == 'drop':\n",
    "            sales[idx] *= np.random.uniform(0.1, 0.5)  # Sharp drop\n",
    "        elif anomaly_type == 'spike':\n",
    "            sales[idx] *= np.random.uniform(1.8, 3.0)  # Sharp spike\n",
    "        elif anomaly_type == 'level_shift':\n",
    "            # Level shift affecting multiple days\n",
    "            shift_length = np.random.randint(3, 10)\n",
    "            if idx + shift_length < n_samples:\n",
    "                sales[idx:idx+shift_length] *= np.random.uniform(1.5, 2.0)\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame({\n",
    "        'date': dates,\n",
    "        'sales': sales,\n",
    "    })\n",
    "    \n",
    "    # Add features for detection\n",
    "    df['day_of_week'] = df['date'].dt.dayofweek\n",
    "    df['month'] = df['date'].dt.month\n",
    "    df['day'] = df['date'].dt.day\n",
    "    df['year'] = df['date'].dt.year\n",
    "    \n",
    "    # Create lag features\n",
    "    for lag in [1, 7, 14]:\n",
    "        df[f'sales_lag_{lag}'] = df['sales'].shift(lag)\n",
    "    \n",
    "    # Create rolling stats\n",
    "    for window in [7, 14, 30]:\n",
    "        df[f'sales_rolling_mean_{window}'] = df['sales'].rolling(window=window).mean()\n",
    "        df[f'sales_rolling_std_{window}'] = df['sales'].rolling(window=window).std()\n",
    "    \n",
    "    # Create sales velocity and acceleration\n",
    "    df['sales_velocity'] = df['sales'] - df['sales_lag_1']\n",
    "    df['sales_acceleration'] = df['sales_velocity'] - df['sales_velocity'].shift(1)\n",
    "    \n",
    "    # Drop rows with NaN (first 30 days due to lag and rolling features)\n",
    "    df = df.dropna().reset_index(drop=True)\n",
    "    \n",
    "    return df, anomaly_indices\n",
    "\n",
    "def detect_anomalies(df, contamination=0.05):\n",
    "    \"\"\"\n",
    "    Detect anomalies in sales data using Isolation Forest\n",
    "    \"\"\"\n",
    "    # Select features for anomaly detection\n",
    "    feature_columns = [col for col in df.columns if col not in ['date']]\n",
    "    \n",
    "    # Fill any remaining NaN values with column means\n",
    "    df_features = df[feature_columns].fillna(df[feature_columns].mean())\n",
    "    \n",
    "    # Standardize the features\n",
    "    scaler = StandardScaler()\n",
    "    df_scaled = scaler.fit_transform(df_features)\n",
    "    \n",
    "    # Train isolation forest model\n",
    "    model = IsolationForest(\n",
    "        contamination=contamination,\n",
    "        random_state=42,\n",
    "        n_estimators=100\n",
    "    )\n",
    "    \n",
    "    # Fit and predict\n",
    "    df['anomaly'] = model.fit_predict(df_scaled)\n",
    "    df['anomaly_score'] = model.decision_function(df_scaled)\n",
    "    \n",
    "    # Convert to binary and score (lower score = more anomalous)\n",
    "    df['anomaly'] = np.where(df['anomaly'] == -1, 1, 0)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def get_feature_importance(df):\n",
    "    \"\"\"\n",
    "    Calculate importance of each feature for anomaly detection\n",
    "    using a Random Forest model\n",
    "    \"\"\"\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    \n",
    "    feature_columns = [col for col in df.columns if col not in ['date', 'anomaly', 'anomaly_score']]\n",
    "    X = df[feature_columns]\n",
    "    y = df['anomaly']\n",
    "    \n",
    "    # Train a random forest classifier\n",
    "    rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    rf.fit(X, y)\n",
    "    \n",
    "    # Get feature importance\n",
    "    importance_df = pd.DataFrame({\n",
    "        'Feature': feature_columns,\n",
    "        'Importance': rf.feature_importances_\n",
    "    }).sort_values('Importance', ascending=False)\n",
    "    \n",
    "    return importance_df\n",
    "\n",
    "def visualize_anomalies(df):\n",
    "    \"\"\"\n",
    "    Visualize the detected anomalies\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(15, 8))\n",
    "    \n",
    "    # Plot sales\n",
    "    plt.plot(df['date'], df['sales'], label='Sales', color='blue', alpha=0.7)\n",
    "    \n",
    "    # Highlight anomalies\n",
    "    anomalies = df[df['anomaly'] == 1]\n",
    "    plt.scatter(anomalies['date'], anomalies['sales'], color='red', \n",
    "                label=f'Anomalies ({len(anomalies)} detected)', s=50)\n",
    "    \n",
    "    plt.title('Sales Data with Detected Anomalies', fontsize=16)\n",
    "    plt.xlabel('Date', fontsize=12)\n",
    "    plt.ylabel('Sales', fontsize=12)\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    return plt\n",
    "\n",
    "def analyze_anomalies(df):\n",
    "    \"\"\"\n",
    "    Analyze detected anomalies and provide insights\n",
    "    \"\"\"\n",
    "    anomalies = df[df['anomaly'] == 1]\n",
    "    \n",
    "    # Group anomalies by day of week\n",
    "    dow_distribution = anomalies['day_of_week'].value_counts().sort_index()\n",
    "    dow_names = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "    dow_distribution.index = [dow_names[i] for i in dow_distribution.index]\n",
    "    \n",
    "    # Group anomalies by month\n",
    "    month_distribution = anomalies['month'].value_counts().sort_index()\n",
    "    month_names = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
    "    month_distribution.index = [month_names[i-1] for i in month_distribution.index]\n",
    "    \n",
    "    # Calculate basic statistics for anomalies vs normal\n",
    "    normal = df[df['anomaly'] == 0]\n",
    "    \n",
    "    stats = {\n",
    "        'avg_sales_anomalies': anomalies['sales'].mean(),\n",
    "        'avg_sales_normal': normal['sales'].mean(),\n",
    "        'std_sales_anomalies': anomalies['sales'].std(),\n",
    "        'std_sales_normal': normal['sales'].std(),\n",
    "        'max_sales_anomalies': anomalies['sales'].max(),\n",
    "        'min_sales_anomalies': anomalies['sales'].min(),\n",
    "        'dow_distribution': dow_distribution,\n",
    "        'month_distribution': month_distribution\n",
    "    }\n",
    "    \n",
    "    return stats, anomalies\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to run the anomaly detection pipeline\n",
    "    \"\"\"\n",
    "    print(\"Generating sample sales data...\")\n",
    "    df, true_anomaly_indices = generate_sample_data(n_samples=730, anomaly_percentage=0.05)\n",
    "    \n",
    "    print(f\"Data shape: {df.shape}\")\n",
    "    print(\"\\nFirst few rows of the data:\")\n",
    "    print(df.head())\n",
    "    \n",
    "    print(\"\\nDetecting anomalies using Isolation Forest...\")\n",
    "    results_df = detect_anomalies(df, contamination=0.05)\n",
    "    \n",
    "    # Get anomaly analysis\n",
    "    stats, anomalies = analyze_anomalies(results_df)\n",
    "    \n",
    "    print(f\"\\nDetected {len(anomalies)} anomalies out of {len(results_df)} data points\")\n",
    "    print(f\"Average sales on normal days: {stats['avg_sales_normal']:.2f}\")\n",
    "    print(f\"Average sales on anomalous days: {stats['avg_sales_anomalies']:.2f}\")\n",
    "    \n",
    "    print(\"\\nDay of week distribution for anomalies:\")\n",
    "    print(stats['dow_distribution'])\n",
    "    \n",
    "    print(\"\\nMonth distribution for anomalies:\")\n",
    "    print(stats['month_distribution'])\n",
    "    \n",
    "    print(\"\\nTop anomalies by anomaly score:\")\n",
    "    top_anomalies = anomalies.sort_values('anomaly_score').head(10)\n",
    "    print(top_anomalies[['date', 'sales', 'anomaly_score']].to_string(index=False))\n",
    "    \n",
    "    # Get feature importance for the anomaly detection\n",
    "    importance_df = get_feature_importance(results_df)\n",
    "    print(\"\\nFeature importance for anomaly detection:\")\n",
    "    print(importance_df.head(10).to_string(index=False))\n",
    "    \n",
    "    # Visualize the results\n",
    "    plt = visualize_anomalies(results_df)\n",
    "    plt.show()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2**: Using clustering algorithms to detect duplicate records where entries are not\n",
    "exactly identical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your code from here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 3**: Implementing classification models to validate data based on learned\n",
    "characteristics from labeled datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your code from here\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
