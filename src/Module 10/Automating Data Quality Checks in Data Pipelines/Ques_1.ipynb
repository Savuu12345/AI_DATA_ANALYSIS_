{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Integrating with Apache Airflow\n",
    "**Description**: Integrate Great Expectations with Apache Airflow to run data quality checks automatically in your DAG.\n",
    "\n",
    "**Steps**:\n",
    "1. Install Airflow (if you haven't already):\n",
    "2. Airflow DAG Integration:\n",
    "    - Create a DAG file:\n",
    "3. Deploy and Test:\n",
    "    - Place this file in your Airflow DAGs directory and start your Airflow scheduler.\n",
    "    - Open the Airflow UI and trigger the DAG to see it run your expectations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'great_expectations'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mairflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moperators\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PythonOperator\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatetime\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m datetime\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgreat_expectations\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mgx\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'great_expectations'"
     ]
    }
   ],
   "source": [
    "# Write your code from here\n",
    "#write \n",
    "from airflow import DAG\n",
    "from airflow.operators.python import PythonOperator\n",
    "from datetime import datetime\n",
    "import great_expectations as gx\n",
    "import pandas as pd\n",
    "import os\n",
    "default_args = {\n",
    "    'owner': 'airflow',\n",
    "    'depends_on_past': False,\n",
    "    'start_date': datetime(2023, 1, 1),\n",
    "    'retries': 1,\n",
    "}\n",
    "dag = DAG(\n",
    "    'great_expectations_data_quality_check',\n",
    "    default_args=default_args,\n",
    "    schedule=None, \n",
    "    catchup=False,\n",
    ")\n",
    "def run_great_expectations_validation():\n",
    "    context = gx.get_context()\n",
    "    data = {'Name': ['Alice', 'Bob', 'Charlie', 'David', 'Eve', None],\n",
    "            'Email': ['alice@example.com', 'bob@example.com', 'charlie@example.com', 'david.example', 'eve@example.com', ''],\n",
    "            'Age': [25, 30, 22, None, 28, 31]}\n",
    "    df = pd.DataFrame(data)\n",
    "    csv_file_path = 'airflow_data_quality.csv'\n",
    "    df.to_csv(csv_file_path, index=False)\n",
    "    datasource_name = 'airflow_pandas_datasource'\n",
    "    data_connector_name = 'default_inferred_data_connector_name'\n",
    "    data_asset_name = 'airflow_data_quality.csv'\n",
    "    expectation_suite_name = 'airflow_data_quality_expectations'\n",
    "    try:\n",
    "        context.get_datasource(datasource_name)\n",
    "        print(f\"Datasource '{datasource_name}' already exists.\")\n",
    "    except gx.exceptions.DatasourceNotFoundError:\n",
    "        pandas_datasource = context.add_pandas_csv_datasource(\n",
    "            name=datasource_name,\n",
    "            base_directory=os.path.dirname(os.path.abspath(csv_file_path)),\n",
    "            batching_regex=r\"(.+)\\.csv\",\n",
    "        )\n",
    "        print(f\"Datasource '{datasource_name}' added.\")\n",
    "    try:\n",
    "        expectation_suite = context.suites.get(expectation_suite_name)\n",
    "        print(f\"Loaded existing Expectation Suite: {expectation_suite_name}\")\n",
    "    except gx.exceptions.ExpectationSuiteNotFoundError:\n",
    "        expectation_suite = context.create_expectation_suite(\n",
    "            expectation_suite_name=expectation_suite_name, overwrite_existing=True\n",
    "        )\n",
    "        validator = context.get_validator(\n",
    "            batch_request=gx.core.batch_request.BatchRequest(\n",
    "                datasource_name=datasource_name,\n",
    "                data_connector_name=data_connector_name,\n",
    "                data_asset_name=data_asset_name,\n",
    "                batch_spec_passthrough={\"reader_method\": \"csv\", \"path_or_buf\": csv_file_path},\n",
    "            ),\n",
    "            expectation_suite=expectation_suite,\n",
    "        )\n",
    "        validator.expect_column_to_exist('Name')\n",
    "        validator.expect_column_to_exist('Email')\n",
    "        validator.expect_column_to_exist('Age')\n",
    "        validator.expect_column_values_to_not_be_null('Email')\n",
    "        validator.save_expectation_suite()\n",
    "        print(f\"Created and saved Expectation Suite: {expectation_suite_name}\")\n",
    "    else:\n",
    "        validator = context.get_validator(\n",
    "            batch_request=gx.core.batch_request.BatchRequest(\n",
    "                datasource_name=datasource_name,\n",
    "                data_connector_name=data_connector_name,\n",
    "                data_asset_name=data_asset_name,\n",
    "                batch_spec_passthrough={\"reader_method\": \"csv\", \"path_or_buf\": csv_file_path},\n",
    "            ),\n",
    "            expectation_suite=expectation_suite,\n",
    "        )\n",
    "    print(f\"Using validator for data asset: {validator.active_batch_request.data_asset_name}\")\n",
    "    validation_result = validator.validate()\n",
    "    print(\"\\nValidation Results:\")\n",
    "    print(validation_result)\n",
    "    context.build_data_docs(validation_result_list=[validation_result])\n",
    "    print(\"Data Docs generated. Check your Airflow logs for the Data Docs path.\")\n",
    "    os.remove(csv_file_path)\n",
    "run_ge_task = PythonOperator(\n",
    "    task_id='run_great_expectations_validation',\n",
    "    python_callable=run_great_expectations_validation,\n",
    "    dag=dag,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
