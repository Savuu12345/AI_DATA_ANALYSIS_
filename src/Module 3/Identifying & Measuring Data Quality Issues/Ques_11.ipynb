{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing email addresses: 2\n",
      "Number of missing department entries: 2\n",
      "Percentage of missing transaction dates: 40.00%\n",
      "Number of duplicate customer records: 1\n",
      "Number of repeated supplier names: 2\n",
      "Number of duplicate product IDs: 1\n",
      "Inconsistent date formats:    order_id order_date\n",
      "3         4        NaT\n",
      "Inconsistent phone number formats:   contact_name phone_number\n",
      "4          Eve      4567890\n",
      "State abbreviation discrepancies:   customer_name               address\n",
      "3         David  101 Maple St, Calif.\n",
      "Average monthly revenue drift: 0.06\n",
      "Average user engagement drift: 0.05\n",
      "Stock price anomalies: 11    0.103448\n",
      "Name: stock_price, dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9494/2138539348.py:105: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  state_discrepancies = df_addresses[~df_addresses['address'].str.contains(r'\\b(CA|California|Calif.)\\b')]\n",
      "/tmp/ipykernel_9494/2138539348.py:140: FutureWarning: 'M' is deprecated and will be removed in a future version, please use 'ME' instead.\n",
      "  'date': pd.date_range(start='2023-01-01', periods=12, freq='M'),\n"
     ]
    }
   ],
   "source": [
    "# Common Data Errors Examples\n",
    "\n",
    "# 1. Missing Data:\n",
    "# Task 1: Review a dataset where some customer emails are missing. Identify how\n",
    "# many records are incomplete.\n",
    "# Task 2: Examine a sales dataset with missing transaction dates and determine the\n",
    "# percentage of missing data.\n",
    "# Task 3: Identify missing department information in an employee registry.\n",
    "import pandas as pd\n",
    "data = {\n",
    "    'customer_id': [1, 2, 3, 4, 5],\n",
    "    'customer_name': ['Alice', 'Bob', 'Charlie', 'David', 'Eve'],\n",
    "    'email': ['alice@example.com', None, 'charlie@example.com', None, 'eve@example.com']\n",
    "}\n",
    "df_customers = pd.DataFrame(data)\n",
    "missing_emails = df_customers['email'].isnull().sum()\n",
    "print(f\"Number of missing email addresses: {missing_emails}\")\n",
    "employee_data = {\n",
    "    'employee_id': [1, 2, 3, 4, 5],\n",
    "    'employee_name': ['John', 'Jane', 'Jim', 'Jack', 'Jill'],\n",
    "    'department': ['HR', None, 'Sales', None, 'IT']\n",
    "}\n",
    "df_employees = pd.DataFrame(employee_data)\n",
    "missing_departments = df_employees['department'].isnull().sum()\n",
    "print(f\"Number of missing department entries: {missing_departments}\")\n",
    "sales_data = {\n",
    "    'transaction_id': [101, 102, 103, 104, 105],\n",
    "    'transaction_date': ['2023-04-01', None, '2023-04-03', None, '2023-04-05'],\n",
    "    'amount': [250, 300, 150, 450, 600]\n",
    "}\n",
    "df_sales = pd.DataFrame(sales_data)\n",
    "\n",
    "missing_dates_percentage = df_sales['transaction_date'].isnull().mean() * 100\n",
    "print(f\"Percentage of missing transaction dates: {missing_dates_percentage:.2f}%\")\n",
    "\n",
    "\n",
    "#\n",
    "import pandas as pd\n",
    "\n",
    "# Task 1: Analyze a customer dataset with duplicate entries and count the number of duplicates\n",
    "customer_data = {\n",
    "    'customer_id': [1, 2, 2, 4, 5],\n",
    "    'customer_name': ['Alice', 'Bob', 'Bob', 'David', 'Eve'],\n",
    "    'email': ['alice@example.com', 'bob@example.com', 'bob@example.com', 'david@example.com', 'eve@example.com']\n",
    "}\n",
    "\n",
    "df_customers = pd.DataFrame(customer_data)\n",
    "duplicates_customers = df_customers.duplicated().sum()\n",
    "print(f\"Number of duplicate customer records: {duplicates_customers}\")\n",
    "\n",
    "# Task 2: Review supplier data and identify any repeated supplier names\n",
    "supplier_data = {\n",
    "    'supplier_id': [1, 2, 3, 4, 5],\n",
    "    'supplier_name': ['ABC Corp', 'XYZ Ltd', 'ABC Corp', 'LMN Inc', 'XYZ Ltd']\n",
    "}\n",
    "\n",
    "df_suppliers = pd.DataFrame(supplier_data)\n",
    "duplicates_suppliers = df_suppliers['supplier_name'].duplicated().sum()\n",
    "print(f\"Number of repeated supplier names: {duplicates_suppliers}\")\n",
    "\n",
    "# Task 3: Examine a product inventory list for duplicates in product IDs\n",
    "product_data = {\n",
    "    'product_id': [101, 102, 103, 102, 105],\n",
    "    'product_name': ['Laptop', 'Mouse', 'Keyboard', 'Mouse', 'Monitor']\n",
    "}\n",
    "\n",
    "df_products = pd.DataFrame(product_data)\n",
    "duplicates_products = df_products['product_id'].duplicated().sum()\n",
    "print(f\"Number of duplicate product IDs: {duplicates_products}\")\n",
    "\n",
    "\n",
    "\n",
    "#\n",
    "import pandas as pd\n",
    "\n",
    "# Task 1: Spot inconsistencies in date formats (e.g., DD/MM/YYYY vs. MM/DD/YYYY)\n",
    "date_data = {\n",
    "    'order_id': [1, 2, 3, 4, 5],\n",
    "    'order_date': ['12/01/2023', '01/12/2023', '03/05/2023', '2023-04-10', '05/11/2023']\n",
    "}\n",
    "\n",
    "df_dates = pd.DataFrame(date_data)\n",
    "df_dates['order_date'] = pd.to_datetime(df_dates['order_date'], errors='coerce')\n",
    "inconsistent_dates = df_dates[df_dates['order_date'].isna()]\n",
    "print(f\"Inconsistent date formats: {inconsistent_dates}\")\n",
    "\n",
    "# Task 2: Identify phone numbers with varying formats in a contact list\n",
    "phone_data = {\n",
    "    'contact_name': ['Alice', 'Bob', 'Charlie', 'David', 'Eve'],\n",
    "    'phone_number': ['123-456-7890', '(123) 456-7890', '1234567890', '123.456.7890', '456-7890']\n",
    "}\n",
    "\n",
    "df_phones = pd.DataFrame(phone_data)\n",
    "df_phones['phone_number'] = df_phones['phone_number'].str.replace(r'\\D', '', regex=True)\n",
    "inconsistent_phones = df_phones[df_phones['phone_number'].str.len() != 10]\n",
    "print(f\"Inconsistent phone number formats: {inconsistent_phones}\")\n",
    "\n",
    "# Task 3: Review address data for discrepancies in state abbreviations (e.g., CA vs. Calif.)\n",
    "address_data = {\n",
    "    'customer_name': ['Alice', 'Bob', 'Charlie', 'David', 'Eve'],\n",
    "    'address': ['123 Main St, CA', '456 Oak St, California', '789 Pine St, CA', '101 Maple St, Calif.', '202 Birch St, CA']\n",
    "}\n",
    "\n",
    "df_addresses = pd.DataFrame(address_data)\n",
    "state_discrepancies = df_addresses[~df_addresses['address'].str.contains(r'\\b(CA|California|Calif.)\\b')]\n",
    "print(f\"State abbreviation discrepancies: {state_discrepancies}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Task 1: Compare monthly revenues over six months to identify data drift\n",
    "revenue_data = {\n",
    "    'month': ['January', 'February', 'March', 'April', 'May', 'June'],\n",
    "    'revenue': [5000, 5100, 5200, 4900, 5300, 6000]\n",
    "}\n",
    "\n",
    "df_revenue = pd.DataFrame(revenue_data)\n",
    "revenue_diff = df_revenue['revenue'].pct_change().dropna()\n",
    "data_drift_revenue = revenue_diff.abs().mean()\n",
    "print(f\"Average monthly revenue drift: {data_drift_revenue:.2f}\")\n",
    "\n",
    "# Task 2: Analyze user engagement metrics from a web application over different quarters\n",
    "user_engagement_data = {\n",
    "    'quarter': ['Q1', 'Q2', 'Q3', 'Q4'],\n",
    "    'active_users': [1200, 1300, 1250, 1400],\n",
    "    'page_views': [4500, 4600, 4700, 4900]\n",
    "}\n",
    "\n",
    "df_engagement = pd.DataFrame(user_engagement_data)\n",
    "user_engagement_diff = df_engagement[['active_users', 'page_views']].pct_change().dropna()\n",
    "data_drift_engagement = user_engagement_diff.abs().mean()\n",
    "print(f\"Average user engagement drift: {data_drift_engagement.mean():.2f}\")\n",
    "\n",
    "# Task 3: Review a stock price dataset to detect any anomalies over a year\n",
    "stock_data = {\n",
    "    'date': pd.date_range(start='2023-01-01', periods=12, freq='M'),\n",
    "    'stock_price': [100, 105, 110, 115, 120, 130, 125, 135, 140, 150, 145, 160]\n",
    "}\n",
    "\n",
    "df_stock = pd.DataFrame(stock_data)\n",
    "stock_diff = df_stock['stock_price'].pct_change().dropna()\n",
    "anomalies_stock = stock_diff[stock_diff.abs() > 0.1]  # Threshold for anomalies (10%)\n",
    "print(f\"Stock price anomalies: {anomalies_stock}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
